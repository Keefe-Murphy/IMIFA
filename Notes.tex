\documentclass[a4paper,12pt,fleqn]{article}
%\documentclass[a4paper,12pt,twocolumn]{article}
\usepackage[margin=0.65in]{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath,amsthm,amssymb,mathtools}
\usepackage{algorithm}% http://ctan.org/pkg/algorithm
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{breqn}
\usepackage{bbm}
\usepackage{lipsum}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{enumitem}
\usepackage[stable]{footmisc}
\setlength{\mathindent}{0pt}
\def\Item$#1${\item $\displaystyle#1$
	\hfill\refstepcounter{equation}(\theequation)}
\def\ItemNN$#1${\item $\displaystyle#1$}
%usepackage[backend=bibtex,bibstyle=numeric,citestyle=numeric]{biblatex}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage[round]{natbib}
\bibliographystyle{myplainnat}
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}} \newcommand{\diagentry}[1]{\mathmakebox[1.8em]{#1}}
\usepackage{nicefrac}
\usepackage[toc,page]{appendix}
\usepackage{array}
\usepackage{chngcntr}
\counterwithin{table}{section}
\counterwithin{figure}{section}
\usepackage{multirow}
\usepackage[font=footnotesize]{caption}
\usepackage[lotdepth,lofdepth]{subfig}
\usepackage{graphicx}
\usepackage{float}
% command to number equations according to the their sections
\numberwithin{equation}{section}
\graphicspath{ {C:/Users/Windows/Dropbox/UCD/UCD_Staff/Publications/PCRC/} }
\usepackage{longtable}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\def\given{\,|\,}

\title{Infinite Mixtures of Infinite Factor Analysers \\ \large Notes \& Derivations}
\author[1, 2]{Keefe Murphy}
\author[1, 2]{Dr. Claire Gormley}
\author[1, 2]{Prof. Brendan Murphy}
\affil[1]{School of Mathematics and Statistics, UCD}
\affil[2]{Insight Centre for Data Analytics, UCD}
%\renewcommand\Authands{ and }

\date{}
\begin{document}
	\nocite{*}
	\maketitle
	%\begin{abstract}
	%\end{abstract}
	\newpage
	\begin{small}
	\tableofcontents
	\end{small}
	\begin{footnotesize}
		%\addcontentsline{toc}{section}{\listtablename}
		%\listoftables
		%\addcontentsline{toc}{section}{\listfigurename}
		%\listoffigures
	\end{footnotesize}
	\newpage
	
\section[Introduction]{Introduction}
\subsection[Model Set-Up]{Model Set-Up}
Let $\underline{x} = \left(x_1, x_2, \ldots, x_p\right)^T$ have mean $\underline{\mu}$ and covariance matrix $\Sigma$. The factor model states that $\underline{x}$ is linearly dependent upon a few $\left(q\ll{p}\right)$ unobservable random variables $\underline{\eta}_i,\underline{\eta}_i,\ldots,\underline{\eta}_q$ called \textit{common factors} and $p$ additional sources of variation $\varepsilon_1,\varepsilon_2,\ldots,\varepsilon_p$ called \textit{specific factors}, for $i=1,\ldots,N$ observations, s.t.
\newline

\noindent\begin{tabular}{l l l l l l}
& $\underline{x}_i$ & $=$ & $\underline{\mu} + \Lambda\underline{\eta}_i + \underline{\varepsilon}_i$&\\
	\\
	where  & $\underline{x}_i$ & $\rightarrow$ & $\left(p \times 1\right)$ & observation vector \\
	& $\underline{\mu}$ &  $\rightarrow$ & $\left(p \times 1\right)$  & overall mean vector \\
	& $\Lambda$ &  $\rightarrow$ & $\left(p \times q\right)$  & loadings matrix \\
	& $\underline{\eta}_i$ &  $\rightarrow$ & $\left(q \times 1\right)$  & vector of factor scores for obs $i$ \\
	& $\underline{\varepsilon}_i$ & $\rightarrow$ & $\left(p \times 1\right)$  & vector of errors for obs $i$ \\
\end{tabular}
\\ \\
$\Lambda_{jk}$ is the \textit{factor loading} of the $j$-th variable on the $k$-th factor of the $\left(p \times q\right) \Lambda$ matrix of factor loadings. If we assume the data have been centred to have column means of 0 then we have
\begin{equation}
\label{eq:1}
\left(\underline{x}_i - \underline{\mu}_{}\right)_{\left(p \times 1\right)} = \underline{x}_{i_{\left(p \times 1\right)}}^\star = \Lambda_{_{\left(p \times q\right)}}\underline{\eta}_{i_{\left(q \times 1\right)}} + \underline{\varepsilon}_{i_{\left(p \times 1\right)}}
\end{equation}

\subsection[Assumptions]{Assumptions}
\begin{enumerate}
	\item $\underline{\mu}=0$
	\item $\underline{\varepsilon}_i$ and $\underline{\eta}_i$ are independent: $\mathrm{Cov}\left(\underline{\eta}_i,\underline{\varepsilon}_i\right) = \mathrm{E}\left(\underline{\eta}_i\underline{\varepsilon}_i^T\right) = 0$
	\item $\underline{\varepsilon}_i \sim\operatorname{MVN}_p\left(\underline{0},\Psi\right)$ where $\Psi = \textit{diag}\left(\psi_1,\ldots,\psi_p\right)$\\
	\\
	$\therefore \mathrm{E}\left(\underline{\varepsilon}_i\right) = \underline{0}$ and $ \mathrm{Cov}\left(\underline{\varepsilon}_i\right) = \begin{pmatrix}
	\diagentry{\psi_1} & 0 & \ldots & 0\\
	0 & \diagentry{\psi_2}& \ldots & 0\\
	\vdots & \vdots & \diagentry{\ddots}& \vdots\\
	0 & 0 & \ldots & \diagentry{\psi_p}
	\end{pmatrix} = \Psi$
	\begin{equation}
	\label{eq:2}
	\therefore \underline{\varepsilon}_i \sim \operatorname{MVN}_p\left(\underline{0},\Psi\right)
	\end{equation}
	\item $\underline{\eta}_i \sim\operatorname{MVN}_q\left(\underline{0}, \mathcal{I}_q\right)$ 
	\\
	$\therefore \mathrm{E}\left(\underline{\eta}_i\right) = \underline{0}$ and $ \mathrm{Cov}\left(\underline{\eta}_i\right) = \begin{pmatrix}
	\diagentry{1} & 0 & \ldots & 0\\
	0 & \diagentry{1}& \ldots & 0\\
	\vdots & \vdots & \diagentry{\ddots}& \vdots\\
	0 & 0 & \ldots & \diagentry{1}
	\end{pmatrix} = \mathcal{I}_q$
	\begin{equation}
	\label{eq:3}
	\therefore \underline{\eta}_i \sim \operatorname{MVN}_q\left(\underline{0}, \mathcal{I}_q\right)
	\end{equation}
\end{enumerate}

\section[Bayesian Framework]{Bayesian Framework}
\subsection[Likelihood]{Likelihood}
\begin{flalign}
	\mathrm{E}\left(\underline{x}_i^\star\right) & = \mathrm{E}\left(\Lambda\underline{\eta}_i + \underline{\varepsilon}_i\right) \nonumber&\\
	& = \Lambda\mathrm{E}\left(\underline{\eta}_i\right) + \mathrm{E}\left(\underline{\varepsilon}_i\right) \nonumber&\\
	& = \underline{0} \nonumber&\\
	\label{eq:4}
	\therefore \underline{x}_i^\star & \sim   \operatorname{MVN}_p\left(\underline{0},\Sigma\right)&\\
	\nonumber\\
	\mbox{Since} \hspace{2mm}  \underline{\varepsilon}_i & = \underline{x}_i^\star -\Lambda\underline{\eta}_i,\nonumber&\\
	\Sigma & = \mathrm{Cov}\left(x_i\right)\nonumber&\\
	& = \mathrm{E}\left[\left(\underline{x}_i-\underline{\mu}_i\right)\left(\underline{x}_i-\underline{\mu}_i\right)^T\right]\nonumber&\\
	& = \mathrm{E}\left[\underline{x}_i^\star \underline{x}_i^{\star^{T}}\right]\nonumber&\\
	& =  \mathrm{E}\left[\left(\Lambda\underline{\eta}_i+\underline{\varepsilon}_i\right)\left(\Lambda\underline{\eta}_i+\underline{\varepsilon}_i\right)^T\right]\nonumber&\\
	& =  \mathrm{E}\left[\left(\Lambda\underline{\eta}_i\right) +
	\underline{\varepsilon}_i\left(\Lambda\underline{\eta}_i\right)^T + \left(\Lambda\underline{\eta}_i\right)\underline{\varepsilon}_i^T + \underline{\varepsilon}_i\underline{\varepsilon}_i^T\right]\nonumber&\\
	& = \Lambda\mathrm{E}\left(\underline{\eta}_i\underline{\eta}_i^T\right)\Lambda^T + \mathrm{E}\left(\underline{\varepsilon}_i\underline{\eta}_i^T\right)\Lambda^T + \Lambda\mathrm{E}\left(\underline{\eta}_i\underline{\varepsilon}_i^T\right) + \mathrm{E}\left(\underline{\varepsilon}_i\underline{\varepsilon}_i^T\right)\nonumber&\\
	& = \Lambda\Lambda^T + \Psi \nonumber&\\
	\label{eq:5} \therefore \underline{x}_i^\star & \sim  \operatorname{MVN}_p\left(\underline{0},\Lambda\Lambda^T + \Psi\right)&\\
	\nonumber\\
	\mathrm{E}\left(\underline{x}_i^\star \given \underline{\eta}_i\right) & = \mathrm{E}\left(\Lambda\underline{\eta}_i + \underline{\varepsilon}_i \given \underline{\eta}_i\right)\nonumber\\
	& =	\Lambda\mathrm{E}\left(\underline{\eta}_i \given \underline{\eta}_i\right) + \mathrm{E}\left(\underline{\varepsilon}_i \given \underline{\eta}_i\right)\nonumber&\\
	& = \Lambda\underline{\eta}_i\nonumber&\\
	\mathrm{Cov}\left(\underline{x}_i^\star \given \underline{\eta}_i\right) & = \mathrm{E}\left[\left(\underline{x}_i^\star - \Lambda\underline{\eta}_i\right)\left(\underline{x}_i^\star - \Lambda\underline{\eta}_i\right)^T \given \underline{\eta}_i\right]\nonumber&\\
	& = \mathrm{E}\left(\underline{\varepsilon}_i\underline{\varepsilon}_i^T \given \underline{\eta}_i\right)\nonumber&\\
	& = \Psi\nonumber&\\
	\label{eq:6} \therefore \underline{x}_i^\star \given \underline{\eta}_i & \sim  \operatorname{MVN}_p\left(\Lambda\underline{\eta}_i,\Psi\right)&\\
	\intertext{The density of the data is then given by:}
	\label{eq:7}\mathrm{P}\left(\underline{x}_i^\star 
	\given \underline{\eta}_i, \Lambda,\Psi\right) & = \left(2\pi\right)^{-\frac{p}{2}} 
	\given\Psi\given^{-\frac{1}{2}} \exp\left(-\frac{1}{2}\sum_{i=1}^{N}\left(\underline{x}_i^\star - \Lambda\underline{\eta}_i\right)^T\Psi^{-1}\left(\underline{x}_i^\star - \Lambda\underline{\eta}_i\right)\right)&\\
	& \propto \given\Psi\given^{-\frac{1}{2}} \exp\left(-\frac{1}{2}\mathrm{tr}\left[\Psi^{-1}\left(X - \eta\Lambda\right)^T\left(X - \eta\Lambda\right)\right]\right)\nonumber&\\
	\mbox{where} \hspace{2mm} \Lambda_{\left(p \times q\right)} & = \begin{pmatrix}
	\diagentry{\lambda_{11}} & \lambda_{12} & \ldots & \lambda_{1q}\\
	\lambda_{21} & \diagentry{\lambda_{22}}& \ldots & \lambda_{2q}\\
	\vdots & \vdots & \diagentry{\ddots}& \vdots\\
	\lambda_{p1} & \lambda_{p2} & \ldots & \diagentry{\lambda_{pq}}
	\end{pmatrix}\nonumber&\\
	\& \hspace{2mm} \eta_{\left(n \times q\right)} & = \begin{pmatrix}
	\diagentry{\eta_{11}} & \eta_{12} & \ldots & \eta_{1q}\\
	\eta_{21} & \diagentry{\eta_{22}}& \ldots & \eta_{2q}\\
	\vdots & \vdots & \diagentry{\ddots}& \vdots\\
	\eta_{n1} & \eta_{n2} & \ldots & \diagentry{\eta_{nq}}
	\end{pmatrix} \& \hspace{2mm}\underline{\eta}_i \hspace{2mm} 
	\mbox{is a column vector containing the entries of row $i$ of $\eta$}\nonumber&
\end{flalign}

\subsection[Posterior Set-Up]{Posterior Set-Up}
\begin{flalign}
	\mbox{Likelihood} \hspace{2mm} &= \prod_{i=1}^N\mathrm{P}\left(\underline{x}_i^\star \given \theta\right)\nonumber &\\
	&= \prod_{i=1}^N\mathrm{P}\left(\underline{x}_i^\star \given \underline{\eta}_i, \Lambda,\Psi\right)\nonumber&\\
	\label{eq:8}
	\mbox{where}~\mathrm{P}\left(\underline{x}_i^\star \given \underline{\eta}_i, \Lambda,\Psi\right) &\sim  \operatorname{MVN}_p\left(\Lambda\underline{\eta}_i,\Psi\right)&\\
	\mbox{Prior} \hspace{2mm}&=\mathrm{P}\left(\theta\right)\nonumber&\\
	 &= \mathrm{P}\left(\eta\right)\mathrm{P}\left(\Lambda\right)\mathrm{P}\left(\Psi\right)\nonumber &\\
	 \mbox{Posterior} \hspace{2mm} &= \mbox{Likelihood} \times \mbox{Prior}&\nonumber\\
	 \therefore \mathrm{P}\left(\eta, \Lambda,\Psi
	  \given X^\star\right) 
	  &\propto \mathcal{L}\left(X^\star \given \eta, \Lambda, \Psi\right) \mathrm{P}\left(\eta\right)\mathrm{P}\left(\Lambda\right)\mathrm{P}\left(\Psi\right)\nonumber&\\
	  &\label{eq:9}\propto \left[\prod_{i=1}^{N}\mathrm{P}\left(\underline{x}_i^\star \given \underline{\eta}_i, \Lambda,\Psi\right)\right]
	  \left[\prod_{i=1}^{N}\mathrm{P}\left(\underline{\eta}_i\right)\right] \left[\prod_{j=1}^{p}\mathrm{P}\left(\underline{\Lambda}_j\right)\right]\left[\prod_{j=1}^{p}\mathrm{P}\left(\psi_j\right)\right]&
	 \end{flalign}
	 
Later on, especially as we move into the mixture case, it will be necessary to undo the centering, thereby removing the $^\star$ on $\underline{x}_i^\star$,  and reintroduce $\underline{\mu}$. This will necessitate multiplying the quantity in \eqref{eq:9} by $\mathrm{P}\left(\underline{\mu}\right)$. However, we will proceed to derive the full conditionals we need for Gibbs Sampling using the centered notation for now as adjusting for $\underline{\mu}$ afterwards will be trivial.
\section[Sampling from the Full Conditionals]{Sampling from the Full Conditionals}
\subsection[Factor Scores]{Factor Scores - $\underline{\eta}_i$}
\begin{flalign}
\underline{\eta}_i &\sim\operatorname{MVN}_q\left(\underline{0}, \mathcal{I}_q\right)\nonumber&\\
\label{eq:10}& = \left(2\pi\right)^{-\frac{q}{2}}\exp\left(-\frac{1}{2}\underline{\eta}_i^T\underline{\eta}_i\right)&
\end{flalign}
To obtain the full conditional for $\underline{\eta}_i$ we can multiply the likelihood by the prior in \eqref{eq:10} s.t.
\begin{flalign}
\mathrm{P}\left(\underline{\eta}_i \given , \underline{x}_i^\star,\Lambda,\Psi\right) &\sim \mathrm{P}\left(\underline{x}_i^\star \given \underline{\eta}_i, \Lambda,\Psi\right)\mathrm{P}\left(\underline{\eta}_i\right)\nonumber&\\
& \propto \exp\left(-\frac{1}{2}\left[\left(\underline{x}_i^\star - \Lambda\underline{\eta}_i\right)^T\Psi^{-1}\left(\underline{x}_i^\star - \Lambda\underline{\eta}_i\right) + \underline{\eta}_i^T\underline{\eta}_i\right]\right)\nonumber&\\
& \propto \exp\left(-\frac{1}{2}\left[-\underline{x}_i^{\star^ {T}}\Psi^{-1}\Lambda\underline{\eta}_i 
	  - \left(\Lambda\underline{\eta}_i\right)^T\Psi^{-1}\underline{x}_i^\star 
	  + \left(\Lambda\underline{\eta}_i\right)^T\Psi^{-1}\left(\Lambda\underline{\eta}_i\right)
	  + \underline{\eta}_i^T\underline{\eta}_i\right]\right)\nonumber&\\
\label{eq:11}& \propto \exp\left(-\frac{1}{2}
	  \left\{\underline{\eta}_i^T\left[\mathcal{I}_q + \Lambda^T\Psi^{-1}\Lambda\right]\underline{\eta}_i\right\} + \underline{x}_i^{\star^ {T}}\Psi^{-1}\Lambda\underline{\eta}_i \right)&\\
	  \intertext{As this is the product of two $\operatorname{MVN}$ distributions we can expect the result to also be $\operatorname{MVN}$.\newline Typically,}
	  \operatorname{MVN}\left(\mu,\Sigma\right) & \propto \exp\left(-\frac{1}{2}\left(\underline{x}-\underline{\mu}\right)^T\Sigma^{-1}\left(\underline{x}-\underline{\mu}\right)\right)\nonumber&\\
& = \exp\left(-\frac{1}{2}\left(\underline{x}^T\Sigma^{-1}\underline{x} -2\underline{\mu}^T\Sigma^{-1}\underline{x} + \underline{\mu}^T\underline{\Sigma}^{-1}\underline{\mu}\right)\right)\nonumber&
\label{eq:12}\intertext{We can identify the $\mu$ and $\Sigma^{-1}$ terms from \eqref{eq:11} above to yield} 
\mathrm{P}\left(\underline{\eta}_i \given \underline{x}_i^\star,\Lambda,\Psi\right) &\sim  \operatorname{MVN}_q\left(\left[\mathcal{I}_q + \Lambda^T\Psi^{-1}\Lambda\right]^{-1}\Lambda^T\Psi^{-1}\underline{x}_i^\star,\left[\mathcal{I}_q + \Lambda^T\Psi^{-1}\Lambda\right]^{-1}\right)&
\end{flalign}
\noindent However, we can reintroduce $\underline{\mu}$ and save on computational time if we implement the algorithm of \citet{GMRFbook}\footnote{To sample $x\sim\operatorname{N}\left(\mu, \Omega^{-1}\right)$, find a matrix $U$ -- non-unique, and square or `tall' -- via Cholesky Decomposition s.t. $U^TU=\Omega$, sample from $z\sim\operatorname{N}\left(0, 1\right)$, then backsolve $L^Tv = Uv = z$ s.t $x=\mu+v=\mu+L^{-T}z=\mu+U^{-1}z.$ Then$\colon$\begin{itemize}\item $\mathrm{E}\left(x\right)= \mu + U^{-1}\mathrm{E}\left(z\right)=\mu$\item $\mathrm{Cov}\left(x,x\right)=\mathrm{Cov}\left(L^{-T},z\right)=\left(L^TL\right)^{-1}=\Omega^{-1}$\end{itemize}}. In fact, we can extend this to block update the scores, thereby obviating the need to loop over $i$:
\begin{itemize}
	\item Calculate $\Omega_\eta = \mathcal{I}_q + \Lambda^T\Psi^{-1}\Lambda$
	\item Compute the Cholesky Factorization $\Omega_\eta = U^TU$.
	\item Sample $\underline{z} \sim \operatorname{MVN}_q\left(\underline{0}, \mathcal{I}_q\right)~N$ times.
	\item Backsolve $U\underline{v} = \underline{z}^T$.
	\item Compute $\Omega_\eta^{-1}$ from $U$.
	\Item $\mbox{Return}~\left(\Omega_\eta^{-1}\Lambda^T\Psi^{-1}\left(C_n\underline{\mu}X\right)^T + \underline{v}\right)^T$\newline where $C_n = \mathcal{I}_n - \frac{1}{n}\mathcal{O} $ and $\mathcal{O}$ is an $N\times N$ matrix of all $1$'s. \label{eq:13}
	\end{itemize}

\subsection[Loadings Matrix]{Loadings Matrix - $\Lambda$}
\label{Loadings_Section}A Gaussian distribution is a conjugate prior for $\Lambda$, implying an $\operatorname{MVN}_q$ distribution prior for each row $\underline{\Lambda}_j$ of $\Lambda$ s.t. $\underline{\Lambda}_j \sim \operatorname{MVN}_q\left(\underline{0},\Sigma_{\lambda}\right)$ where $\Sigma_{\lambda}$ is a diagonal covariance matrix. As above, we can expect the result of the product of two $\operatorname{MVN}_q$ distributions to itself be distributed this way.
\begin{flalign}
\mathrm{P}\left(\underline{\Lambda}_j\given X^\star,\eta,\Psi\right) &\sim \mathrm{P}\left(X^\star \given \eta, \underline{\Lambda}_j,\Psi\right)\mathrm{P}\left(\underline{\Lambda}_j\given\Sigma_{\lambda}\right)\nonumber&\\
& \propto \exp\left(-\frac{1}{2}\sum_{i=1}^{N}\left(\underline{x}_i^\star - \underline{\Lambda}_j\underline{\eta}_i\right)^T\psi_j^{-1}\left(\underline{x}_i^\star - \underline{\Lambda}_j\underline{\eta}_i\right)\right)\exp\left(-\frac{1}{2}\left(\underline{\Lambda}_j^T\Sigma_{\lambda}^{-1}\underline{\Lambda}_j\right)\right) \nonumber&\\
& \propto \exp\left(-\frac{1}{2}\sum_{i=1}^{N}\left[- 2\underline{x}_i^{\star^{T}}\psi_j^{-1}\left(\underline{\Lambda}_j\underline{\eta}_i\right) + \left(\underline{\Lambda}_j\underline{\eta}_i\right)^T\psi_j^{-1}\left(\underline{\Lambda}_j\underline{\eta}_i\right) + \underline{\Lambda}_j^T\Sigma_{\lambda}^{-1}
\underline{\Lambda}_j\right]\right)\nonumber&\\
& \propto \exp\left(\underline{\Lambda}_j\psi_j^{-1}\sum_{i=1}^{N}x_{ij}^{\star^{T}}\underline{\eta}_i - \frac{1}{2}\underline{\Lambda}_j^T\left[\sum_{i=1}^{N}\psi_j^{-1}\underline{\eta}_i^T\underline{\eta}_i\right]\underline{\Lambda}_j -\frac{1}{2} \underline{\Lambda}_j^T\Sigma_{\lambda}^{-1}\underline{\Lambda}_j\right)\nonumber&\\
\label{eq:14}& \propto \exp\left(\underline{\Lambda}_j\left[\eta^T\psi_j^{-1}\underline{x}^{j^\star}\right] - \frac{1}{2}\underline{\Lambda}_j^T\left[\Sigma_{\lambda}^{-1} + \psi_j^{-1}\eta^T\eta\right]\underline{\Lambda}_j\right)&
\end{flalign}
where $\underline{x}^{j^\star}$ is an $N$-vector containing the elements of the $j$-th column of $X^\star$.
\begin{equation}
\begin{split}
	\therefore \mathrm{P}\left(\underline{\Lambda}_j\given X^\star,\eta,\Psi\right) &\sim&\hspace{-3mm} \operatorname{MVN}_q\Big(\left[\Sigma_{\lambda}^{-1} + \psi_j^{-1}\eta^T\eta\right]^{-1}\eta^T\psi_j^{-1}\underline{x}^{j^\star},\\&&\left[\Sigma_{\lambda}^{-1} + \psi_j^{-1}\eta^T\eta\right]^{-1}\Big)&\label{eq:15}
\end{split}
\end{equation}
\newpage
\noindent However, we can reintroduce $\underline{\mu}$ and save on computational time, as before, if we:
\begin{itemize}
	\item Calculate $\Omega_{\lambda_j} = \Sigma_{\lambda}^{-1} + \psi_j^{-1}\eta^T\eta$.
	\item Compute the Cholesky Factorization $\Omega_{\lambda_j} = U^TU$.
	\item Sample $\underline{z} \sim \operatorname{MVN}_q\left(\underline{0}, \mathcal{I}_q\right)$.
	\item Back-solve $U\underline{v} = \underline{z}$.
	\item Compute $\Omega_{\lambda_j}^{-1}$ from $U$.
	\Item $\mbox{Return}~\Omega_{\lambda_j}^{-1}\eta^T\psi_j^{-1}\left(\underline{x}^{j} - \underline{1}\mu_j\right) + \underline{v}$\newline
	where $\underline{1}$ is an $N$-vector of all $1$'s. \label{eq:16}
\end{itemize}

\subsection[Uniquenesses]{Uniquenesses - $\Psi$}
\begin{flalign}
\shortintertext{If we suggest an Inverse Wishart prior distribution for $\Psi$, we have:}
\mathrm{P}\left(\Psi\right) & \propto \given\Psi^{-1}\given^{\frac{N + p + 1}{2}}\exp\left(-\frac{1}{2}\mathrm{tr}\left(\mathcal{S}^{-1^\star}\Psi\right)\right)&\nonumber\\
\shortintertext{Using the fact that
$\mathrm{V}^{-1} \sim \operatorname{Wish}_p\left(\nu, \Sigma\right)~\mbox{when}~ \mathrm{V} \sim \operatorname{Wish}_p^{-1}\left(m, \Sigma^{-1}\right)~\mbox{with}~m = \nu + p + 1$ we get:}
\mathrm{P}\left(\Psi^{-1}\right) & \propto \given\Psi^{-1}\given^{\frac{N}{2}}\exp\left(-\frac{1}{2}\mathrm{tr}\left(\mathcal{S}^\star\Psi^{-1}\right)\right)\nonumber
\shortintertext{Since $\Psi$ is a diagonal matrix$\colon$}
\mathrm{P}\left(\Psi^{-1}\right) & \propto \prod_{j=1}^{p}\given\psi_j^{-1}\given^{\frac{N}{2}}\exp\left(-\frac{1}{2}\mathrm{tr}\left(\mathcal{S}_j^\star\psi_j^{-1}\right)\right)\nonumber
\shortintertext{This suggests the prior for $\Psi^{-1}$ is a product of $p \hspace{2mm} \operatorname{Ga}\left(\alpha,\beta\right)$ distributions. We choose hyperparameters as per \cite{Fruhwirth-Schnatter2010-II}, by bounding each $\psi_j$ away from zero in such a way that Heywood problems are avoided.}
\therefore \mathrm{P}\left(\Psi^{-1} \given \alpha, \beta \right) & = \prod_{j=1}^{p}\mathrm{P}\left(\psi_j^{-1} \given \alpha, \beta \right)&\nonumber \\
& \propto \prod_{j=1}^{p} \left(\psi_j^{-1}\right)^{\alpha - 1}\exp\left(- \beta\psi_j^{-1}\right)\nonumber&\\
\therefore \mathrm{P}\left(\Psi^{-1}\given X^\star, \eta, \Lambda\right) &\propto \mathrm{P}\left(X^\star \given \eta,\Lambda\right)\mathrm{P}\left(\Psi^{-1} \given \alpha,\beta\right) \nonumber&\\
&\propto \prod_{j=1}^{p}\left(\psi_j^{-1}\right)^{\frac{N}{2}}\exp\left(-\frac{\mathcal{S}_j^\star}{2}\psi_j^{-1}\right)\prod_{j=1}^{p}\left(\psi_j^{-1}\right)^{\alpha - 1}\exp\left(-\beta\psi_j^{-1}\right)\nonumber&\\
& \propto \prod_{j=1}^{p}\left(\psi_j^{-1}\right)^{\frac{N}{2} + \alpha - 1}\exp\left(-\bigg(\frac{\mathcal{S}_j^\star}{2} + \beta\bigg)\psi_j^{-1}\right)\label{eq:17}\\
\mbox{where} \hspace{2mm}\mathcal{S}_j^\star &=  \sum_{i=1}^{N}\left(x_{ij} - \underline{\Lambda}_j\underline{\eta}_i\right)^2\nonumber 
\shortintertext{However, we can reintroduce $\underline{\mu}$ at this stage by rewriting:}
\mathcal{S}_j &=  \sum_{i=1}^{N}\left(x_{ij} - \mu_j - \underline{\Lambda}_j\underline{\eta}_i\right)^2 \nonumber&
\shortintertext{Thus the posterior distribution of each $\psi_j^{-1}$ is given by:}
\mathrm{P}\left(\psi_j^{-1}\given X,F,\Lambda\right) & \sim \operatorname{Ga}\left(\alpha + \frac{N}{2},\beta + \frac{S_j}{2}\right)&\label{eq:18}
\end{flalign}
\subsection[Reintroducing $\mu$]{Reintroducing $\underline{\mu}$}
We've already seen from \eqref{eq:13}, \eqref{eq:16} and \eqref{eq:18} that reintroducing $\mu$ to the other full conditionals is trivial. All that remains is to specify the conjugate Gaussian prior for $\mu$ itself, and to derive its full conditional. This implies an $\operatorname{MVN}_p$ distribution prior s.t. $\underline{\mu} \sim \operatorname{MVN}_p\left(\underline{\tilde{\mu}}, \Sigma_{\mu}\right)$ where $\Sigma_{\mu}$ is a diagonal covariance matrix, typically the diagonal of the data covariance matrix, and $\underline{\tilde{\mu}}$ is a vector of prior mean means, typically the sample mean for each group. As above, we can expect the result of the product of two $\operatorname{MVN}_p$ distributions to itself be distributed this way.
\begin{flalign}
\mathrm{P}\left(\underline{\mu}\given X,\eta,\Psi, \Lambda\right)
& \propto  \exp\left(-\frac{1}{2}\sum_{i=1}^{N}\left(\underline{x}_i - \underline{\mu} - \Lambda\underline{\eta}_i\right)^T\Psi^{-1}\left(\underline{x}_i - \underline{\mu} - \Lambda\underline{\eta}_i\right)\right)\exp\left(-\frac{1}{2}\left(\underline{\mu} - \underline{\tilde{\mu}}\right)^T\Sigma_{\mu}^{-1}\left(\underline{\mu} - \underline{\tilde{\mu}}\right)\right) \nonumber&\\
& \propto \exp\left(-\frac{1}{2}\bigg(\sum_{i=1}^{N}\left[-2 \underline{x}_i^T\Psi^{-1}\underline{\mu} + 2\left(\Lambda\underline{\eta}_i\right)^T\Psi^{-1}\underline{\mu}  + \underline{\mu}^T\Psi^{-1}\underline{\mu}\right] +\underline{\mu}^T \Sigma_{\mu}^{-1}\underline{\mu} - 2\underline{\tilde{\mu}}^T\Sigma_{\mu}^{-1}\underline{\mu}\bigg)\right)\nonumber&\\
& \propto  \exp\left(\sum_{i=1}^{N}\underline{x}_i^T\Psi^{-1}\underline{\mu} -\sum_{i=1}^{N}\left(\Lambda\underline{\eta}_i\right)^T\Psi^{-1}\underline{\mu} -\frac{1}{2}\left[\underline{\mu}^T\left(\Sigma_{\mu}^{-1} + N\Psi^{-1}\right)\underline{\mu}\right] + \underline{\tilde{\mu}}^T\Sigma_{\mu}^{-1}\underline{\mu}\right)\nonumber&
\end{flalign}
\vspace{-5mm}
\begin{equation}
	\begin{split}
	\therefore \mathrm{P}\left(\underline{\mu}\given X,\eta,\Psi,\Lambda\right)&\sim&\hspace{-3mm}\operatorname{MVN}_p\Bigg(\left[\Sigma_{\mu}^{-1} + N\Psi^{-1}\right]^{-1}\Big(\Psi^{-1}\big(\sum_{i=1}^{N}\underline{x}_i - \sum_{i=1}^{N}\Lambda\underline{\eta}_i\big) + \Sigma_{\mu}^{-1}\underline{\tilde{\mu}}\Big), \\&& \left[\Sigma_{\mu}^{-1} + N\Psi^{-1}\right]^{-1}\Bigg)\label{eq:19}
	\end{split}
\end{equation}
\noindent However, we can save on computational time, as before, if we:
\begin{itemize}
	\item Calculate $\Omega_{\mu} = \Sigma_{\mu}^{-1} + N\Psi^{-1}$, which is a diagonal $p\times p$ matrix.
	\item Invert $\Omega_{\mu}$ by inverting its diagonal elements.
	\item $\Omega_{\mu}^{-1} = U^TU$ can be obtained by taking the square root of $\Omega_{\mu}$ since this matrix is diagonal.
	\item Sample $\underline{z} \sim \operatorname{MVN}_p\left(\underline{0}, \mathcal{I}_p\right)$.
	\item Compute $\underline{v} = U^T\underline{z}$.
	\Item $\mbox{Return}~\Omega_{\mu}^{-1}\Big(\Psi^{-1}\big(\sum_{i=1}^{N}\underline{x}_i - \sum_{i=1}^{N}\Lambda\underline{\eta}_i\big) + \Sigma_{\mu}^{-1}\underline{\tilde{\mu}}\Big) + \underline{v}$.
	\label{eq:20}
\end{itemize}

\subsection[Gibbs Sampler Pseudo-Code]{Gibbs Sampler Pseudo-Code}
\newcounter{eqn}
\renewcommand*{\theequation}{\roman{eqn})}
\newcommand{\num}{\refstepcounter{eqn}\text{\theequation}\quad}

\newcounter{loop}[eqn]
\renewcommand*{\thepart}{\alph{loop})}
\newcommand{\alphloop}{\refstepcounter{loop}\text{\thepart}\quad}
\label{Gibbs1}
	\begin{alignat*}{7}
	\intertext{\num Choose hyperparameters $\Sigma_{\mu}, \Sigma_{\lambda}, \alpha, \mbox{and}~ \beta$, select $q$ and initialise $\underline{\tilde{\mu}}$.}
	\num& \mbox{Initalise:} \quad& &\underline{\mu}^{\left(0\right)} &~\sim~& \operatorname{MVN}_p\left(\underline{\tilde{\mu}}, \Sigma_{\mu}\right) &\\
	&\quad& &\underline{\eta}_i^{\left(0\right)} &~\sim~&\operatorname{MVN}_q\left(\underline{0},\mathcal{I}_q\right)~\quad\hspace{1.5mm}\forall~i = 1,\ldots,N&\\
	&\quad& &\underline{\Lambda}_j^{\left(0\right)} &~\sim~&\operatorname{MVN}_q\left(\underline{0},\Sigma_{\lambda}\right)~\quad\forall~j = 1,\ldots,p&\\
	&\quad& &\psi_j^{-1^{\left(0\right)}} &~\sim~& \operatorname{Ga}\left(\alpha,\beta\right)~\quad\hspace{8mm}\forall~j = 1,\ldots,p
	\intertext{\num For $t = 1, \ldots, T$, using the routines specified in \eqref{eq:13}, \eqref{eq:16}, \eqref{eq:18} and \eqref{eq:20}$\colon$}
	&\quad& \alphloop&\Omega_{\mu}^{\left(t\right)} &~=~& \Sigma_{\mu}^{-1} + N\Psi^{-1^{\left(t-1\right)}}&\\
	&\quad& &\underline{\mu}^{\left(t\right)} &~\sim~& \operatorname{MVN}_p\left(\Omega_{\mu}^{-1^{\left(t\right)}}\Big(\Psi^{-1}\big(\sum_{i=1}^{N}\underline{x}_i - \sum_{i=1}^{N}\Lambda\underline{\eta}_i\big) + \Sigma_{\mu}^{-1}\underline{\tilde{\mu}}\Big),\Omega_{\mu}^{-1^{\left(t\right)}}\right)&\\
	&\quad& \alphloop&\Omega_\eta^{\left(t\right)} &~=~&\mathcal{I}_q + \Lambda^{T^{\left(t-1\right)}}\Psi^{-1^{\left(t-1\right)}}\Lambda^{\left(t-1\right)}&\\
		&\quad & & \underline{\eta}_i^{\left(t\right)} &~\sim~& \operatorname{MVN}_q\left(\Omega_\eta^{-1^{\left(t\right)}}\Lambda^{T^{\left(t-1\right)}}\Psi^{-1^{\left(t-1\right)}}\left(\underline{x}_i -\underline{\mu}^{\left(t\right)}\right),\Omega_\eta^{-1^{\left(t\right)}}\right)&\\
	&\quad &\alphloop &\mbox{For}~j &~=~& 1, \ldots, p&\\
	&\quad & \bullet~&\Omega_{\lambda_j}^{\left(t\right)} &~=~& \Sigma_{\lambda}^{-1} + \psi_j^{-1^{\left(t-1\right)}}\eta^{T^{\left(t\right)}}\eta^{\left(t\right)}&\\
	&\quad & &  \underline{\Lambda}_j^{\left(t\right)} &~\sim~& \operatorname{MVN}_q\left(\Omega_{\lambda_j}^{-1^{\left(t\right)}}\eta^{T^{\left(t\right)}}\psi_j^{-1^{\left(t-1\right)}}\left(\underline{x}^j -\underline{1}\mu_j^{\left(t\right)}\right),\Omega_{\lambda_j}^{-1^{\left(t\right)}}\right)&\\
	&\quad &\bullet~&  \psi_j^{-1^{\left(t\right)}} &~\sim~& \operatorname{Ga}\left(\alpha + \frac{N}{2},\beta + \frac{S_j^{\left(t\right)}}{2}\right)
	\intertext{\num Disregard the first $\operatorname{B}$ burn-in iterations and thin every $\operatorname{K}$-th iteration.}
	\intertext{\num Calculate the log-likelihood for each remaining sample. Then, using the largest value observed across these draws, BIC-MCMC, as defined by \citet{Fruhwirth-Schnatter2011}, is determined by $2\ln\hat{\mathcal{L}} - k\ln\left(N\right)$, where $k = pq -\frac{q\left(q-1\right)}{2} + 2p$ is the effective number of parameters in the model. When choosing between competing models, the one with the highest BIC-MCMC is preferred. Alternatively, AIC-MCMC, or the BICM and AICM of \cite{Raftery2007} can be used.}&
	\end{alignat*}	
	\renewcommand*{\theequation}{\arabic{equation}}
	\numberwithin{equation}{section}
	\vspace{-20mm}
\subsection[Issues Around Identifiability]{Issues Around Identifiability}
	Most covariance matrices $\Sigma$ cannot be uniquely factored as $\Lambda\Lambda^T + \Psi$ where $q \ll{p}$. Let $T$ be any $q\times q$ orthogonal matrix such that $TT^T = \mathcal{I}_q$. Then$\colon$ 
	\begin{flalign}
	\underline{x}_i- \underline{\mu} &= \Lambda\underline{\eta}_i + \underline{\varepsilon}_i\nonumber&\\
	&= \Lambda TT^T\underline{\eta}_i + \underline{\varepsilon}_i\nonumber&\\
	&= \Lambda^\star\underline{\eta}_i^\star + \underline{\varepsilon}_i\nonumber
	\intertext{where $\Lambda^\star = \Lambda T$ and $\underline{\eta}_i^\star = T^T\underline{\eta}_i$.
	It follows that $\mathrm{E}\left(\underline{\eta}_i^\star\right) = \underline{0}$ and $\mathrm{Cov}\left(\underline{\eta}_i^\star\right) = \mathcal{I}_q$.
	Thus it is impossible, given the data $X$, to distinguish between $\Lambda$ and $\Lambda^\star$ since they both generate the same covariance matrix $\Sigma\colon$}
	\Sigma &= \Lambda\Lambda^T + \Psi\nonumber&\\
	&= \Lambda TT^T\Lambda^T + \Psi\nonumber&\\
	&= \Lambda^\star\Lambda^{\star^{T}} + \Psi\nonumber&
	\end{flalign}
	However, we can address this identifiability problem, using Procrustean methods, by mapping each iteration's loadings matrix to a common `template' loadings matrix --- which we have taken to be the loadings matrix at the end of the burn-in period. This Procustean map is a rotation only, i.e. translation, scaling, dilation, etc. are not permitted. We then also apply that same rotation matrix at each iteration to each sample of the matrix of factor scores. This amounts to \textit{post-multiplying} the loadings and factor score matrices at each iteration by the Procrustes rotation matrix that maps to that iteration's loadings template.
	
\section[Introducing the Shrinkage Prior]{Introducing the Shrinkage Prior}
\subsection[Multiplicative Gamma Process Shrinkage Priors]{Multiplicative Gamma Process Shrinkage Priors}
\label{MGP} We now propose the multiplicative gamma process shrinkage prior of \citet{Bhattacharya2011} on the factor loadings which allows the introduction of infinitely many factors, with the loadings increasingly shrunk towards zero as the column index increases. Their prior is placed on a parameter expanded factor loadings matrix without imposing any restriction on the loading elements, thereby making the induced prior on the covariance matrix invariant to the ordering of the data. The Gibbs sampler can still be used due to the joint conjugacy property of this prior, which allows block updating of the loadings matrix. Furthermore, these authors propose that an adaptive Gibbs sampler be used for automatically truncating the infinite loading matrix, through selection of the number of important factors, to one having finite columns. This facilitates posterior computation while providing a close approximation of the infinite factor model.

The exact specification of this shrinkage-type prior allows the degree of shrinkage to increase across the column index as follows$\colon$
\begin{flalign}
\lambda_{jk} \given \phi_{jk},\tau_k &\sim \mathrm{N}\left(0,\phi_{jk}^{-1}\tau_k^{-1}\right)\nonumber&\\
\mbox{s.t.}\quad \underline{\lambda}_j \given \underline{\phi}_j,\underline{\tau}&\sim \operatorname{MVN}_{q^\star}\left(\underline{0},\mathrm{D}_j\right)\label{eq:21}&\\
\mbox{where}\quad\mathrm{D}_j^{-1} &= \operatorname{diag}\left(\phi_{j1}\tau_1,\ldots,\phi_{jq^\star}\tau_{q^\star}\right)\nonumber&\\
\vspace{2mm}\phi_{jk} &\sim \operatorname{Ga}\left(\nu,\nu\right)\label{eq:22}&\\
\vspace{2mm}\tau_k &= \prod_{h=1}^k \delta_h\nonumber&\\
\delta_1 &\sim \operatorname{Ga}\left(\alpha_1,\beta_1\right), \quad\delta_h \sim \operatorname{Ga}\left(\alpha_k,\beta_k\right), \quad h \geq 2\label{eq:23}&
\end{flalign}
\noindent where $\delta_h \left(h=1,\ldots,\infty\right)$ are independent, $\tau_k$ is a \textit{global} shrinkage parameter for the $k$-th column and the $\phi_{jk}$s are \textit{local} shrinkage parameters for the elements in the $k$-th column. The $\tau_k$s are stochastically increasing under the restriction $\alpha_k > 1$, which favours more shrinkage as the column index increases. Typically $\beta_1 = \beta_k = 1$.

\subsection[Defining new MGP Full Conditionals]{Defining new MGP Full Conditionals}
We propose a Gibbs sampler for posterior computation, much like the one above, after truncating the loadings matrix to have $q^\star \ll p$ columns. An adaptive strategy for inference on the truncation level $q^\star$ is described in \ref{Adapt_Section}. For now, let's focus on the new full conditionals for the loadings matrix, global shrinkages, and local shrinkages which need to be derived in order to implement this. Once again, these parameters are initialised according to their priors. The other full conditionals are exactly as before, with just a small adjustment to the factor scores to allow for the truncation to $q^\star$ columns, i.e. $\mathrm{P}\left(\underline{\eta}_i \given \mbox{---}\right) \sim  \operatorname{MVN}_{q^\star}\left(\left[\mathcal{I}_{q^\star} + \Lambda^T_{q^\star}\Psi^{-1}\Lambda_{q^\star}\right]^{-1}\Lambda^T\Psi^{-1}\underline{x}_i - \underline{\mu},\left[\mathcal{I}_{q^\star} + \Lambda^T_{q^\star}\Psi^{-1}\Lambda_{q^\star}\right]^{-1}\right)$

\subsubsection[Loadings Matrix]{Loadings Matrix - $\Lambda$}
Incorporating the new prior \eqref{eq:21}, and following the same steps as \ref{Loadings_Section} above, it's trivial to show that the $\Lambda_j$s now have independent conditionally conjugate posteriors given by$\colon$
\begin{equation}
\mathrm{P}\left(\Lambda_j\given \mbox{---}\right) \sim \operatorname{MVN}_{q^\star}\left(\left[\mathrm{D}_j^{-1} + \psi_j^{-1}\eta^T\eta\right]^{-1}\eta^T\psi_j^{-1}\underline{x}^{j^\star},\left[\mathrm{D}_j^{-1} + \psi_j^{-1}\eta^T\eta\right]^{-1}\right)\label{eq:24}\end{equation}
\noindent However, we can reintroduce $\underline{\mu}$ and save on computational time, as before, if we follow the routine given in \eqref{eq:16}, with $\Omega_{\lambda_j} = \mathrm{D}_j^{-1} + \psi_j^{-1}\eta^T\eta$.


\subsubsection[Local Shrinkage]{Local Shrinkage -- $\phi_{jk}$}
Using the conditional prior in \eqref{eq:21} and the prior for $\phi_{jk}$ in \eqref{eq:22} we can derive the full conditional for the local shrinkage parameter as follows$\colon$
\begin{flalign}
\mathrm{P}\left(\phi_{jk} \given \mbox{---}\right) &\propto \mathrm{P}\left(\lambda_{jk} \given \phi_{jk},\tau_k\right)\mathrm{P}\left(\phi_{jk}\right)\nonumber\\
&\propto \frac{\phi_{jk}^{\nicefrac{1}{2}}\tau_k^{\nicefrac{1}{2}}}{\sqrt{2\pi}}\exp\left\{-\frac{1}{2}\lambda_{jk}^2\phi_{jk}\tau_k\right\}\phi_{jk}^{\nu-1}\exp\left\{-\nu\phi_{jk}\right\}\nonumber\\
&\propto \phi_{jk}^{\nicefrac{1}{2}}\phi_{jk}^{\nu-1}\exp\left\{\left(-\frac{1}{2}\lambda_{jk}^2\tau_k - \nu\right)\phi_{jk}\right\}\nonumber\\
&\propto \phi_{jk}^{\nu-\nicefrac{1}{2}}\exp\left\{-\frac{1}{2}\left(2\nu + \lambda_{jk}^2\tau_k\right)\phi_{jk}\right\}\nonumber
\intertext{Thus the full conditional for each $\phi_{jk}$ is given by$\colon$}
\mathrm{P}\left(\phi_{jk}\given \mbox{---}\right) &\sim \operatorname{Ga}\left(\nu + \frac{1}{2},\nu + \frac{\tau_k\lambda_{jk}^2}{2}\right)\label{eq:26}
\end{flalign}

\subsubsection[Global Shrinkage]{Global Shrinkage -- $\tau_k$}
Using the conditional prior in \eqref{eq:21} and the prior for $\tau_k$ in \eqref{eq:23} we can derive the full conditional for the global shrinkage parameter, in three stages -- first by deriving and sampling from $\mathrm{P}\left(\delta_1\given\mbox{---}\right)~\&~\mathrm{P}\left(\delta_k\given\mbox{---}\right)$ for $k\geq 2$, as follows below --- and then obtaining the product $\tau_k = \prod_{h=1}^k \delta_h$ thereafter$\colon$
\begin{flalign}
\mathrm{P}\left(\delta_1 \given \mbox{---}\right) & \propto \prod_{j=1}^{p}\prod_{k=1}^{q^\star}\operatorname{N}\left(\lambda_{jk}\given 0, \phi_{jk}^{-1}\tau_k^{-1}\right) \times \operatorname{Ga}\left(\delta_1 \given \alpha_1, \beta_1\right)\nonumber\\
&\propto \prod_{j=1}^p \operatorname{N}\left(\lambda_{j1}\given 0, \phi_{j1}^{-1}\tau_1^{-1}\right)\times\ldots\times\prod_{j=1}^p\operatorname{N}\left(\lambda_{jq^\star}\given 0, \phi_{jq^\star}^{-1}\tau_{q^\star}^{-1}\right)\times \operatorname{Ga}\left(\delta_1\given\alpha_1, \beta_1\right)\nonumber\\
&\propto \left(\phi_{j1}\tau_1\right)^{\nicefrac{p}{2}}\exp\left(-\frac{1}{2}\sum_{j=1}^p\lambda_{j1}^2\phi_{j1}\tau_1\right)\times\ldots\times\left(\phi_{jq^\star}\tau_{q^\star}\right)^{\nicefrac{p}{2}}\exp\left(-\frac{1}{2}\sum_{j=1}^p\lambda_{jq^\star}^2\phi_{jq^\star}\tau_{q^\star}\right)\nonumber\\&\hspace{108mm}\times \delta_1^{\alpha_1-1}\exp\left(-\beta_1\delta_1\right)\nonumber\\
&\propto \left(\phi_{j1}\delta_1\right)^{\nicefrac{p}{2}}\exp\left(-\frac{1}{2}\sum_{j=1}^p\lambda_{j1}^2\phi_{j1}\delta_1\right)\times\ldots\times\left(\phi_{jq^\star}\delta_1\delta_2\ldots\delta_{q^\star}\right)^{\nicefrac{p}{2}}\exp\left(-\frac{1}{2}\sum_{j=1}^p\lambda_{jq^\star}^2\phi_{jq^\star}\delta_1\delta_2\ldots\delta_{q^\star}\right)\nonumber\\&\hspace{135mm}\times \delta_1^{\alpha_1-1}\exp\left(-\beta_1\delta_1\right)\nonumber\\
&\propto\delta_1^{\nicefrac{pq^\star}{2}+\alpha_1-1}\exp\left(-\frac{\delta_1}{2}\left(\sum_{j=1}^p\lambda_{j1}^2\phi_{j1}+\ldots+\lambda_{jq^\star}^2\phi_{jq^\star}\delta_2\ldots\delta_{q^\star} +2\beta_1\right)\right)\nonumber\\
&\propto\delta_1^{\nicefrac{pq^\star}{2}+\alpha_1-1}\exp\left(-\frac{\delta_1}{2}\left(\sum_{h=1}^{q^\star}\tau_h^{\left(1\right)}\sum_{j=1}^p\lambda_{jh}^2\phi_{jh}+2\beta_1\right)\right)\nonumber\\
\mbox{where}~\tau_h^{\left(k\right)} &= \prod_{t=1}^h \frac{\delta_t}{\delta_k}~\mbox{for}~k=1,\ldots,q^\star\label{eq:27}\\
\therefore \mathrm{P}\left(\delta_1\given\mbox{---}\right)& \sim \operatorname{Ga}\left(\alpha_1 + \frac{pq^\star}{2}, \beta_1 + \frac{1}{2}\sum_{h=1}^{q^\star}\tau_h^{\left(1\right)}\sum_{j=1}^p\lambda_{jh}^2\phi_{jh}\right)\label{eq:28}
\end{flalign}
\begin{flalign}
\mathrm{P}\left(\delta_k \given \mbox{---}\right) & \propto \prod_{j=1}^{p}\prod_{k=1}^{q^\star}\operatorname{N}\left(\lambda_{jk}\given 0, \phi_{jk}^{-1}\tau_k^{-1}\right) \times \operatorname{Ga}\left(\delta_k \given \alpha_k, \beta_k\right)\nonumber\\
&\propto \left(\phi_{j1}\delta_1\right)^{\nicefrac{p}{2}}\exp\left(-\frac{1}{2}\sum_{j=1}^p\lambda_{j1}^2\phi_{j1}\delta_1\right)\times\ldots\times\left(\phi_{jq^\star}\delta_1\delta_2\ldots\delta_{q^\star}\right)^{\nicefrac{p}{2}}\exp\left(-\frac{1}{2}\sum_{j=1}^p\lambda_{jq^\star}^2\phi_{jq^\star}\delta_1\delta_2\ldots\delta_{q^\star}\right)\nonumber\\&\hspace{134mm}\times \delta_k^{\alpha_k-1}\exp\left(-\beta_k\delta_k\right)\nonumber\\
&\propto \delta_k^{\nicefrac{p}{2}\left(q^\star - k + 1\right) + \alpha_k - 1}\exp\left(-\frac{\delta_k}{2}\left(\sum_{h=k}^{q^\star}\tau_h^{\left(k\right)}\sum_{j=1}^p\lambda_{jh}^2\phi_{jh} + 2\beta_k\right)\right)\nonumber\\
\therefore \mathrm{P}\left(\delta_k\given \mbox{---}\right) &\sim \operatorname{Ga}\left(\alpha_k + \frac{p}{2}\left(q^\star - k + 1\right),\beta_k +\frac{1}{2}\sum_{h=k}^{q^\star}\tau_h^{\left(k\right)}\sum_{j=1}^p\lambda_{jh}^2\phi_{jh}\right)\label{eq:29}
\end{flalign}
\subsection[Adaptive Step]{Adaptive Step}
\label{Adapt_Section}
In practical situations, we expect to have relatively few important factors compared with the dimension $p$ of the outcomes. The most common approach for selecting the number of factors relies on fitting the finite factor model for different choices of $q^\star$, and then using the BIC, BIC-MCMC, or another model selection criterion. This approach can be difficult to implement for large $p$, small $N$ problems, and the BIC itself isn't well justified for factor models even for small to moderate $p$. However, the infinite factor model obviates the need for pre-specifying the number of factors, while the sparsity favouring prior on the loadings ensures that the effective number of factors would be small when the truth is sparse. However, we need a computational strategy for choosing an appropriate level of truncation $q^\star$. We would like to strike a balance between missing important factors by choosing $q^\star$ too small and wasting computation on an overly high truncation level. One can think of $q^\star$ as the effective number of factors, so that the contribution from adding additional factors is negligible. Starting with a conservative guess $\tilde{q}$ of $q^\star$, the posterior samples of $\Lambda_{\tilde{q}}$ from the Gibbs sampler contain information about the effective number of factors. At the $t$-th iteration, let $m^{\left(t\right)}$ denote the number of columns in $\Lambda_{\tilde{q}}$ having all elements in a pre-specified small neighbourhood of zero. Intuitively, $m^{\left(t\right)}$ of the factors have a negligible contribution at the $t$-th iteration. We then define $q^{\star\left(t\right)} = \tilde{q} - m^{\left(t\right)}$ to be the effective number of factors at iteration $t$. It's typically necessary to choose a very conservative upper-bound to be assured that $\tilde{q} \geq q^\star$, though this leads to wasted computational effort. Ideally, we would like to discard the redundant factors and continue sampling with a reduced number of loadings columns. We thereby save on computation by discarding unimportant factors. For this reason, the sampler described in \ref{Gibbs1} above is modified to an adaptive Gibbs sampler, which tunes the number of factors as the sampler progresses. We begin with a default value for $\tilde{q}$ of $\min\left(\left\lfloor 3\ln(p)\right\rfloor, p, N-1\right)$. We adapt only after the burn-in period has elapsed, in order to ensure we're sampling from the true posterior distribution before truncating the loadings matrix. We adapt with probability $\mathrm{p}\left(t\right) = \exp\left(b_0 + b_1t\right)$ at the $t$-th iteration after burn-in, with $b_0$, $b_1$ chosen so that adaptation occurs around every 10 iterations at the beginning of the chain but decreases in frequency exponentially fast. We chose $b_0$ and $b_1$ in the adaptation probability as $-0.1$ and $-5 \times 10^{-5}$ respectively. We generate a sequence $u_t$ of uniform random numbers between $0$ and $1$. If $u_t \leq \mathrm{p}\left(t\right)$ at the $t$-th iteration, we monitor the columns in the loadings matrix having $75\%$ of elements less than $10^{-1}$ in magnitude. If the number of such columns drops to zero, an additional loadings column is added by simulating from the prior distribution. Otherwise redundant columns are discarded and parameters corresponding to the non-redundant columns are retained. The other parameters are also modified accordingly. Letting $\tilde{q}^{\left(t\right)}$ denote the truncation level at iteration $t$ and $q^{\star\left(t\right)} = \tilde{q}^{\left(t\right)} - m^{\left(t\right)}$ denote the effective number of factors, we use the posterior mode or median of $q^{\star\left(t\right)}$ after burn-in as an estimate of $q^\star$ with credible intervals quantifying uncertainty. Thus a histogram approximation to the posterior for $q^\star$ is introduced and may be used to address the question about the number of latent factors.

\section[Extension to Clustering Heterogeneous Data]{Extension to Clustering Heterogeneous Data}
\subsection[Introducing Mixture Models]{Introducing Mixture Models}
Marginally, \ref{eq:5} provides a parsimonious covariance matrix, i.e.~$\underline{x}_i\given \theta \sim  \operatorname{MVN}_p\left(\underline{\mu},\Lambda\Lambda^T + \Psi\right)$.
This allows us to exploit model-based clustering capabilities in high dimensional data settings. We can employ a(n)~(in)finite mixture of factor analyis models whereby each of the $G$ clusters is modelled using a cluster specific latent Gaussian model with covariance specified according to the form above. Let's now introduce some basic notation at this stage: 
\begin{flalign}
N &=\sum_{g=1}^{G}n_g~\quad\hspace{18mm}~\mbox{where $n_g$ is the size of the $g$-th group.}\nonumber\\
\mathrm{P}\left(X\given\gamma\right) &= \sum_{g=1}^{G}\pi_g\mathrm{P}_g\left(X\given\theta_g\right)~\quad\mbox{where}~\gamma = \left(\theta_1,\ldots,\theta_G,\pi_1,\ldots,\pi_G\right),\\
&\hspace{41mm}\mbox{and the p.d.f $\mathrm{P}_g$ is parametrized by $\theta_g$.}\nonumber
\intertext{The \textit{cluster mixing proportions} - $\pi_1,\ldots,\pi_G$ - have the following properties}
\pi_g &\geq~0~\quad\forall~g = 1,\ldots,G\nonumber\\
\sum_{g=1}^{G}\pi_g &= 1\nonumber
\intertext{Introduce an additional latent indicator $G$-vector of \textit{cluster labels} -- $\underline{z}_i$ -- s.t.}
z_{ig} & =
\begin{cases} 1~\mbox{if}~i \in g\\
0~\mbox{otherwise}\end{cases}\nonumber\\
\intertext{Therefore, if $G=3$, for instance, and observation $i$ belongs to cluster 2, $\underline{z}_i =\left(0,1,0\right)$. Hence,} \underline{x}_i\given z_{ig} = 1 &\sim\operatorname{MVN}_p\left(\underline{\mu}_g,\Lambda_g\Lambda_g^T + \Psi_g\right)\nonumber\\
\therefore \mathrm{P}\left(\underline{x}_i\right) &= \sum_{g=1}^{G}\pi_g\operatorname{MVN}_p\left(\underline{\mu}_g,\Lambda_g\Lambda_g^T + \Psi_g\right)\label{eq:30}
\end{flalign}
\subsubsection[Decomposable Prior for $\gamma$]{Decomposable Prior for $\gamma$}
\begin{flalign}
\shortintertext{The posterior distribution of $\gamma$ is}
\mathrm{P}\left(\gamma\given X\right) & \propto \mathrm{P}\left(\gamma\right)\prod_{i=1}^{N}\mathrm{P}\left(\underline{x}_i\given\gamma\right)\nonumber\\
&\propto \mathrm{P}\left(\gamma\right)\prod_{i=1}^{N}\left(\sum_{g=1}^{G}\pi_g\mathrm{P}_g\left(\underline{x}_i\given\theta_g\right)\right)\nonumber\\
\therefore \mathrm{P}\left(\gamma\given X,Z\right) & \propto \mathrm{P}\left(\gamma\right)\prod_{g=1}^{G}\prod_{i\colon z_{ig} = 1}\mathrm{P}_g\left(\underline{x}_i\given\theta_g\right)\nonumber
\shortintertext{If, $\mathrm{P}\left(\gamma\right)$ can be decomposed into}
\mathrm{P}\left(\gamma\right) &= \mathrm{P}\left(\pi\right)\prod_{g=1}^{G}\mathrm{P}\left(\theta_g\right)\mbox{, then}\nonumber\\
\mathrm{P}\left(\gamma\given X,Z\right) & \propto \mathrm{P}\left(\pi\right)\prod_{g=1}^{G}\prod_{i\colon z_{ig}=1}\mathrm{P}\left(\theta_g\right)\mathrm{P}_g\left(\underline{x}_i\given \theta_g\right) \label{eq:31}
\end{flalign}
\subsection[Deriving Posterior Distributions]{Deriving Posterior Distributions}
Attention now turns towards deriving full conditional distributions for the new parameter $\underline{\pi}$, as well as the latent variables $Z$, so that we can sample them for clustering purposes, by incorporating them into the Adaptive Gibbs Sampler framework described variously above.

\begin{itemize}
	\item Component Parameters -- $\theta_g\colon$	\vspace{2mm}\\
	$\mathrm{P}\left(\theta_g\given\theta_{-g},X,Z\right) \equiv \mathrm{P}\left(\theta_g\given X,Z\right) \propto \prod_{i\colon z_{ig} = 1}\mathrm{P}\left(\theta_g\right)\mathrm{P}_g\left(\underline{x}_i\given\theta_g\right)$\vspace{2mm}\\
	where $\theta_{-g} = \left(\theta_1,\ldots,\theta_{g-1},\theta_{g+1},\ldots,\theta_G\right)$\\
	\item Cluster Mixing Proportions -- $\underline{\pi}\colon$\vspace{2mm}\\
	$\mathrm{P}\left(\underline{\pi}\given X, Z\right) \equiv \mathrm{P}\left(\underline{\pi}\given Z\right) \propto \mathrm{P}\left(\underline{\pi}\right)\prod_{g=1}^{G}\pi_g^{n_g}$\vspace{2mm}\\
	where $n_g$ is the number of observations in group $g$,\vspace{2mm}\\
	since $\mathrm{P}\left(\underline{z}_i\given\underline{\pi}\right) \sim \operatorname{Mult}\left(1, \underline{\pi}\right)$\\
	\item Latent Variables -- $\underline{z}_i\colon$\vspace{2mm}\\
	$\mathrm{P}\left(\underline{z}_i\given\underline{x}_i,\gamma\right) \propto \mathrm{P}\left(\underline{z}_i\right)\mathrm{P}\left(\underline{x}_i\given\theta_{i\colon z_{ig}=1},\underline{z}_i\right)$
\end{itemize}
\subsubsection[Cluster Mixing Proportions]{Cluster Mixing Proportions -- $\underline{\pi}$}
\label{pi_prior}
Let the prior distribution of $\underline{\pi}$ be Dirichlet with parameter $\underline{\alpha}$ -- a multivariate generalisation of the Beta distribution. Typically a symmetric uniform prior is chosen, whereby $\alpha_g = 1~\forall~g=1,\ldots,G$.
\begin{flalign}
\mathrm{P}\left(\underline{\pi}\right) &\propto \prod_{g=1}^{G}\pi_g^{\alpha_g-1}\nonumber\\
\therefore \mathrm{P}\left(\underline{\pi}\given Z, X\right) & \propto \prod_{g=1}^{G}\pi_g^{\alpha_g-1}\prod_{g=1}^{G}\pi_g^{n_g}\nonumber\\
&\propto \prod_{g=1}^{G}\pi_g^{\alpha_g + n_g - 1}\nonumber\\
\mbox{i.e.}~\mathrm{P}\left(\underline{\pi}\given Z, X\right) &\sim \operatorname{Dir}\left(\underline{\alpha}+\underline{n}\right)\label{eq:32}\\
\mbox{where}~\underline{n} &= \left(n_1,\ldots,n_G\right)\nonumber
\end{flalign}
\subsubsection[Latent Variables]{Latent Variables -- $\underline{z}_i$}
\begin{flalign}
\underline{z}_i \given \underline{x}_i,\gamma &\sim \operatorname{Mult}\left(1,\underline{p}\right),~\mbox{where}\nonumber\\
\underline{p} &= \left(p_1,\ldots,p_G\right),~\mbox{and}\nonumber\\
p_g &=~\mathrm{P}\left(\underline{z}_{ig}=1\given\underline{x}_i,\gamma\right) = \frac{\pi_g\mathrm{P}\left(\underline{x}_i\given\theta_g\right)}{\sum_{g=1}^{G}\pi_g\mathrm{P}\left(\underline{x}_i \given\theta_g\right)} = \frac{\pi_g f\left(\underline{x}_i \given \underline{\mu}_g,\Lambda_g\Lambda_g^T+\Psi_g\right)}{\sum_{g=1}^G\pi_g f\left(\underline{x}_i \given \underline{\mu}_g,\Lambda_g\Lambda_g^T+\Psi_g\right)}\label{eq:33}&\\
&=\exp\Big[\log\left(\pi_g\right) + \log\Big(f\big(\underline{x}_i \given \underline{\mu}_g,\Lambda_g\Lambda_g^T+\Psi_g\big)\Big) - \sum_{g=1}^{G}\left(\log\left(\pi_g\right) + \log\Big(f\big(\underline{x}_i \given \underline{\mu}_g,\Lambda_g\Lambda_g^T+\Psi_g\big)\right)\Big]\nonumber
\end{flalign}
\newpage
\subsubsection[Mixtures of Infinite Factor Analyzers Pseudo-Code]{Mixtures of Infinite Factor Analyzers Pseudo-Code}
\begin{enumerate}[label*=\arabic*.]
	\item Choose scalar hyperparameters as before.
	\item Start by initialising the cluster labels $Z^{\left(0\right)}\colon$~simulate from the $\operatorname{Mult}\left(1, \underline{\pi}\right)$ prior $N$ times, or employ another clustering algorithm, such as K-Means. Compute $\underline{n}$, and $\underline{\tilde{\mu}}_g$ for each group.
	\item Initialise, $\forall~g=1,\ldots,G\colon$
		\begin{flalign}
	\underline{\mu}_g^{\left(0\right)} &\sim \operatorname{MVN}_p\left(\underline{\tilde{\mu}}_g, \Sigma_{\mu}\right) &\nonumber\\
	\underline{\eta}_{i}^{\left(0\right)} &\sim\operatorname{MVN}_{q_g^\star}\left(\underline{0},\mathcal{I}_{q_g^\star}\right)~\quad\forall~i = 1,\ldots,N&\nonumber\\
	\underline{\Lambda}_{jg}^{\left(0\right)} &\sim\operatorname{MVN}_{q_g^\star}\left(\underline{0},\Sigma_{\lambda}\right)~\quad\hspace{0.5mm}\forall~j = 1,\ldots,p&\nonumber\\
	\psi_{jg}^{-1^{\left(0\right)}} &\sim \operatorname{Ga}\left(\alpha,\beta\right)~\quad\quad\hspace{6mm}\forall~j = 1,\ldots,p&\nonumber\\
	\phi_{jkg}^{\left(0\right)} &\sim \operatorname{Ga}\left(\nu,\nu\right)~\quad\quad\hspace{6.5mm}\forall~j = 1,\ldots,p\quad\mbox{and}\quad k=1,\ldots,q_g^\star\nonumber\\
	\delta_{1g}^{\left(0\right)} &\sim \operatorname{Ga}\left(\alpha_1,\beta_1\right),\quad\delta_{hg}^{\left(0\right)} \sim \operatorname{Ga}\left(\alpha_k,\beta_k\right),\quad h\geq 2\nonumber\\
	\tau_{kg}^{\left(0\right)} &= \prod_{h=1}^{k}\delta_{hg}^{\left(0\right)}\quad\hspace{16.2mm}\forall~k=1,\ldots,q_g^\star\nonumber
		\end{flalign}
	\item For $g = 1,\ldots,G$, sample other parameters as before, but this time from their \textit{group specific} full conditionals$\colon$
	\begin{alignat*}{4}
		\alphloop&\Omega_{\mu_g}^{\left(t\right)} &=&~\Sigma_{\mu}^{-1} + n_g\Psi_g^{-1^{\left(t-1\right)}}\\
		&\underline{\mu}_g^{\left(t\right)} &\sim&~\operatorname{MVN}_p\left(\Omega_{\mu_g}^{-1^{\left(t\right)}}\Big(\Psi_g^{-1^{\left(t-1\right)}}\big(\sum_{i\colon z_{ig} = 1}\underline{x}_i - \sum_{i\colon z_{ig} = 1}\Lambda_g^{\left(t-1\right)}\underline{\eta}_i^{\left(t-1\right)}\big) + \Sigma_{\mu}^{-1}\underline{\tilde{\mu}}_g\Big),\Omega_{\mu_g}^{-1^{\left(t\right)}}\right)\\
		\alphloop&\Omega_{\eta_g}^{\left(t\right)} &=&~\mathcal{I}_{q_g^\star} + \Lambda_g^{T^{\left(t-1\right)}}\Psi_g^{-1^{\left(t-1\right)}}\Lambda_g^{\left(t-1\right)}\\
		&\underline{\eta}_{i\colon z_{ig} = 1}^{\left(t\right)}~&\sim&~\operatorname{MVN}_q\left(\Omega_{\eta_g}^{-1^{\left(t\right)}}\Lambda_g^{T^{\left(t-1\right)}}\Psi_g^{-1^{\left(t-1\right)}}\left(\underline{x}_{i\colon z_{ig} = 1} -\underline{\mu}_g^{\left(t\right)}\right),\Omega_{\eta_g}^{-1^{\left(t\right)}}\right)\\
		\alphloop &\mbox{For}~j &=& 1, \ldots, p\\
		\bullet~&\Omega_{\lambda_{jg}}^{\left(t\right)} &=& \mathrm{D}_j^{-1} + \psi_{jg}^{-1^{\left(t-1\right)}}\eta_{i\colon z_{ig} = 1}^{T^{\left(t\right)}}\eta_{i\colon z_{ig} = 1}^{\left(t\right)}\\
		&  \underline{\Lambda}_{jg}^{\left(t\right)} &\sim& \operatorname{MVN}_{q_g^\star}\left(\Omega_{\lambda_{jg}}^{-1^{\left(t\right)}}\eta_{i\colon z_{ig} = 1}^{T^{\left(t\right)}}\psi_{jg}^{-1^{\left(t-1\right)}}\left(\underline{x}_{i\colon z_{ig} = 1}^j -\underline{1}\mu_{jg}^{\left(t\right)}\right),\Omega_{\lambda_{jg}}^{-1^{\left(t\right)}}\right)\\
		\bullet~&  \psi_{jg}^{-1^{\left(t\right)}} &\sim& \operatorname{Ga}\left(\alpha + \frac{n_g}{2},\beta + \frac{S_{jg}^{\left(t\right)}}{2}\right)\\
		\bullet~& \phi_{jkg}^{\left(t\right)} &\sim& \operatorname{Ga}\left(\nu + \frac{1}{2},\nu+\frac{\tau_{kg}^{\left(t-1\right)}\lambda_{jkg}^{2^{\left(t\right)}}}{2}\right)\quad\forall~k=1,\ldots,q_g^\star\\
		\alphloop&\delta_{1g}^{\left(t\right)} &\sim& \operatorname{Ga}\left(\alpha_1 + \frac{pq_g^\star}{2},\beta_1 + \frac{1}{2}\sum_{h=1}^{q_g^\star}\tau_{hg}^{\left(1\right)^{\left(t-1\right)}}\sum_{j=1}^p\lambda_{jhg}^{2^{\left(t\right)}}\phi_{jhg}^{\left(t\right)}\right)\\
		&\delta_{hg}^{\left(t\right)} &\sim& \operatorname{Ga}\left(\alpha_k + \frac{p}{2}\left(q_g^\star-k+1\right), \beta_k + \frac{1}{2}\sum_{h=k}^{q_g^\star}\tau_{hg}^{\left(k\right)^{\left(t\right)}}\sum_{j=1}^p\lambda_{jhg}^{2^{\left(t-1\right)}}\phi_{jhg}^{\left(t\right)}\right),\quad h\geq 2\\
		&\tau_{kg}^{\left(t\right)} &=& \prod_{h=1}^{k}\delta_{hg}^{\left(t\right)}\quad\hspace{38mm}\forall~k=1,\ldots,q_g^\star\nonumber
		\end{alignat*}
	\item Re-compute $\underline{n}$ and sample $\underline{\pi}$ from $\operatorname{Dir}\left(\underline{\alpha} + \underline{n}\right)$.
	\item For $i=1,\ldots,N$, sample $\underline{z}_i$ as outlined in \eqref{eq:33}.
	\item Follow the adaptation procedure outlined in \ref{Adapt_Section}\footnote{Our R-package also implements MFA, without the MGP shrinkage prior in \ref{MGP} and adaptation.}.
	\item Repeat steps 4--7 for $t=2,\ldots,T$ using the current value for $q_g^\star$.
	\item Disregard the first $\operatorname{B}$ burn-in iterations and thin every $\operatorname{K}$-th iteration \footnote{If using the MFA approach, one chooses between competing models according to the pair of G and Q values which optimise one of the model selection criteria outlined in \ref{Gibbs1}. When using the MIFA approach, one chooses G using BICM or AICM only.}.
\end{enumerate}
\subsection[Label Switching]{Label Switching}
It's easy to see that $\mathrm{P}\left(X\given\gamma\right) = \mathrm{P}\left(X\given\tilde\gamma\right)$ where $\tilde\gamma = \left(\theta_{j_1},\ldots,\theta_{j_G},\pi_{j_1},\ldots,\pi_{j_G}\right)$ and $j_1,\ldots,j_G$ is any permutation of $1,\ldots,G$. This type of finite mixture distribution nonidentifiability is caused by the invariance of mixture distributions to component relabelling: by interchanging the order of components, the distributions induced by $\gamma$ and $\tilde{\gamma}$ are the same, although evidently the two parameters are distinct. For finite mixture distribution as defined above with $G$ components, there exist $G!$ equivalent ways of arranging them. Generally as the Markov chain progresses, we will observe switches between these equivalent modes. When the main goal is identifying/interpreting mixture components \&/or clustering, this \textit{label switching} phenomenon needs to be addressed. The approach we adopt to do so is applied post-hoc, after the chain has finished running, and has the advantage of not involving loss functions based on sampled model parameters. We only require samples of $Z$, which are matched to a template vector of cluster labels at burnin using the cost-minimizing permutation suggested by the square assignment algorithm of \cite{CarpToth1980}. This same permutation is applied to all other parameters which vary by group, namely the means, loadings, uniquenesses, and mixing proportions, prior to computing their posterior mean estimates.

\subsection[Overfitting Mixtures]{Overfitting Mixtures}
The need to choose the optimal number of latent factors in a mixture of factor analysers has been obviated using MIFA, but the issue of model choice is still not entirely resolved. Overfitting mixtures, along with Dirichlet Processes (\ref{Dirichlet}), are a means of extending the MIFA methodology in order to estimate $G$ in a similarly choice-free manner. The prior in \ref{pi_prior}  plays an important role. This method approaches mixture model estimation by initially overfitting the number of clusters expected to be present, and specified conditions on the Dirichlet hyperparameter for the cluster mixing proportions encourage the emptying out of excess components in the posterior distribution. 

To initialise the method, a conservatively high number of groups $G^\star$ is chosen, and fixed for the entire length of the MCMC chain. It's assumed that $G^\star > G$. Each $\alpha_g = 0.5/G^\star$ is set small enough to favour empty groups a priori \cite{Ishwaran}. The symmetric uniform prior $\operatorname{Dir}\left(1,\ldots,1\right)$ used previously is rather indifferent in this respect. The number of non-empty groups at each iteration $G_0$ is recorded thusly:
\begin{equation}
	G_0 = G^{\star} - \sum_{g=1}^{G} \mathbbm{1}\big(\sum\limits_{i} z_{ig}=0\big)\label{eq:34}
\end{equation}
The true $G$ is estimated by the $G_0$ value visited most often by the sampler. Component specific inference is conducted only on the $M_0$ samples corresponding to those visits.

\subsection[Dirichlet Process Mixtures]{Dirichlet Process Mixtures}
\label{Dirichlet}
\bibliography{Notes_bibtex}
\end{document}