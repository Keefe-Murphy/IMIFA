@article{Bhattacharya2011,
	abstract = {We focus on sparse modelling of high-dimensional covariance matrices using Bayesian latent factor models. We propose a multiplicative gamma process shrinkage prior on the factor loadings which allows introduction of infinitely many factors, with the loadings increasingly shrunk towards zero as the column index increases. We use our prior on a parameter-expanded loading matrix to avoid the order dependence typical in factor analysis models and develop an efficient Gibbs sampler that scales well as data dimensionality increases. The gain in efficiency is achieved by the joint conjugacy property of the proposed prior, which allows block updating of the loadings matrix. We propose an adaptive Gibbs sampler for automatically truncating the infinite loading matrix through selection of the number of important factors. Theoretical results are provided on the support of the prior and truncation approximation bounds. A fast algorithm is proposed to produce approximate Bayes estimates. Latent factor regression methods are developed for prediction and variable selection in applications with high-dimensional correlated predictors. Operating characteristics are assessed through simulation studies, and the approach is applied to predict survival times from gene expression data.},
	author = {Bhattacharya, A. and Dunson, D. B.},
	doi = {10.1093/biomet/asr013},
	file = {:C$\backslash$:/Users/Windows/Documents/Claire IMIFA/Papers {\&} Books/BhattacharyaDunson.pdf:pdf},
	issn = {00063444},
	journal = {Biometrika},
	keywords = {Adaptive Gibbs sampling,Factor analysis,High-dimensional data,Multiplicative gamma process,Parameter expansion,Regularization,Shrinkage},
	mendeley-groups = {IMIFA},
	number = {2},
	pages = {291--306},
	pmid = {23049129},
	title = {\textit{Sparse Bayesian infinite factor models}},
	volume = {98},
	year = {2011}
}

@article{Bhattacharya2015,
abstract = {We propose an efficient way to sample from a class of structured multivariate Gaussian distributions which routinely arise as conditional posteriors of model parameters that are assigned a conditionally Gaussian prior. The proposed algorithm only requires matrix operations in the form of matrix multiplications and linear system solutions. We exhibit that the computational complexity of the proposed algorithm grows linearly with the dimension unlike existing algorithms relying on Cholesky factorizations with cubic orders of complexity. The algorithm should be broadly applicable in settings where Gaussian scale mixture priors are used on high dimensional model parameters. We provide an illustration through posterior sampling in a high dimensional regression setting with a horseshoe prior on the vector of regression coefficients.},
archivePrefix = {arXiv},
arxivId = {1506.04778},
author = {Bhattacharya, A. and Chakraborty, A. and Mallick, B. K.},
eprint = {1506.04778},
file = {:C$\backslash$:/Users/Keefe/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhattacharya, Chakraborty, Mallick - 2015 - Fast sampling with Gaussian scale-mixture priors in high-dimensional regression.pdf:pdf},
keywords = {able,bayesian,gaussian scale mixture,global-local prior,high dimensional,scal-,shrinkage,sparsity},
mendeley-groups = {IMIFA},
pages = {1--11},
title = {{Fast sampling with Gaussian scale-mixture priors in high-dimensional regression}},
url = {http://arxiv.org/abs/1506.04778},
year = {2015}
}

@article{CarpToth1980,
	author = {G. Carpaneto and P. Toth},
	journal = {ACM Transactions on Mathematical Software},
	mendeley-groups = {IMIFA},
	pages = {104--111},
	title = {\textit{Algorithm 548 Solution of the assignment problem}},
	volume = {6},
	year = {1980}
}

@article{Carvalho2008,
abstract = {We describe studies in molecular profiling and biological pathway analysis that use sparse latent factor and regression models for microarray gene expression data. We discuss breast cancer applications and key aspects of the modeling and computational methodology. Our case studies aim to investigate and characterize heterogeneity of structure related to specific oncogenic pathways, as well as links between aggregate patterns in gene expression profiles and clinical biomarkers. Based on the metaphor of statistically derived "factors" as representing biological "subpathway" structure, we explore the decomposition of fitted sparse factor models into pathway subcomponents and investigate how these components overlay multiple aspects of known biological activity. Our methodology is based on sparsity modeling of multivariate regression, ANOVA, and latent factor models, as well as a class of models that combines all components. Hierarchical sparsity priors address questions of dimension reduction and multiple comparisons, as well as scalability of the methodology. The models include practically relevant non-Gaussian/nonparametric components for latent structure, underlying often quite complex non-Gaussianity in multivariate expression patterns. Model search and fitting are addressed through stochastic simulation and evolutionary stochastic search methods that are exemplified in the oncogenic pathway studies. Supplementary supporting material provides more details of the applications, as well as examples of the use of freely available software tools for implementing the methodology.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Carvalho, C. M. and Chang, J/ and Lucas, J. E. and Nevins, J. R. and Wang, Q. and West, M.},
doi = {10.1198/016214508000000869},
eprint = {NIHMS150003},
file = {:D$\backslash$:/Dropbox/UCD/IMIFA/Papers {\&} Books/Carvalho - Bayesian Factor Regression Models.pdf:pdf},
isbn = {0162-1459 (Print)$\backslash$n0162-1459 (Linking)},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {biological pathways,breast cancer genomics,decomposing gene expression patterns,dirichlet process factor model,evolutionary stochastic search,factor regression,gene expression analysis,gene expression profiling,gene networks,non-gaussian multivariate analysis,sparse factor models,sparsity priors},
mendeley-groups = {IMIFA},
number = {484},
pages = {1438--1456},
pmid = {21218139},
title = {{High-Dimensional Sparse Factor Modeling: Applications in Gene Expression Genomics}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3017385{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {103},
year = {2008}
}

@article{Durante2016,
abstract = {Adaptive dimensionality reduction in high-dimensional problems is a key topic in statistics. The multiplicative gamma process prior takes a step in this direction, but improved studies on its properties are required to ease implementation. This note addresses such aim.},
archivePrefix = {arXiv},
arxivId = {1610.03408},
author = {Durante, D.},
eprint = {1610.03408},
file = {:C$\backslash$:/Users/Keefe/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Durante - 2016 - A note on the multiplicative gamma process.pdf:pdf},
keywords = {matrix factorization,multiplicative gamma process,shrinkage prior,stochastic order},
mendeley-groups = {IMIFA},
title = {{A note on the multiplicative gamma process}},
url = {http://arxiv.org/abs/1610.03408},
year = {2016}
}

@article{Escobar1994,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2291223},
 abstract = {In this article, the Dirichlet process prior is used to provide a nonparametric Bayesian estimate of a vector of normal means. In the past there have been computational difficulties with this model. This article solves the computational difficulties by developing a "Gibbs sampler" algorithm. The estimator developed in this article is then compared to parametric empirical Bayes estimators (PEB) and nonparametric empirical Bayes estimators (NPEB) in a Monte Carlo study. The Monte Carlo study demonstrates that in some conditions the PEB is better than the NPEB and in other conditions the NPEB is better than the PEB. The Monte Carlo study also shows that the estimator developed in this article produces estimates that are about as good as the PEB when the PEB is better and produces estimates that are as good as the NPEB estimator when that method is better.},
 author = {M. D. Escobar},
 journal = {Journal of the American Statistical Association},
 number = {425},
 pages = {268-277},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Estimating Normal Means with a Dirichlet Process Prior},
 volume = {89},
 year = {1994}
}

@article{EscWest1995,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2291069},
 abstract = {We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation and are exemplified by special cases where data are modeled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior, and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models.},
 author = {M. D. Escobar and M. West},
 journal = {Journal of the American Statistical Association},
 number = {430},
 pages = {577-588},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayesian Density Estimation and Inference Using Mixtures},
 volume = {90},
 year = {1995}
}

@article{Ferguson1973,
author = "Ferguson, T. S.",
doi = "10.1214/aos/1176342360",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "03",
number = "2",
pages = "209--230",
publisher = "The Institute of Mathematical Statistics",
title = "A Bayesian Analysis of Some Nonparametric Problems",
url = "http://dx.doi.org/10.1214/aos/1176342360",
volume = "1",
year = "1973"
}

@article{Forina1983,
author = "Forina, M. and Armanino, C. and Lanteri, S. and Tiscornia, E.",
year = "1983",
pages = {{189-214}},
title = "Classification of olive oils from their fatty acid composition",
journal = "Food Research and Data Analysis",
publishers = "Applied Science Publishers, London"
}

@book{Fruhwirth-Schnatter2010,
	author = {Fr{\"{u}}hwirth-Schnatter, S.},
	doi = {10.1007/978-0-387-98135-2},
	file = {:C$\backslash$:/Users/Windows/Documents/Claire IMIFA/Papers {\&} Books/[Fruhwirth-Schnatter] Finite Mixture and Markov Switching Models.pdf:pdf},
	isbn = {9780387775005},
	issn = {01727397},
	publisher = {Springer series in statistics},
	mendeley-groups = {TextBooks},
	pages = {1--656},
	pmid = {15772297},
	title = {{Finite Mixture and Markov Switching Models}},
	year = {2010}
}

@article{Fruhwirth-Schnatter2010-II,
	title = {Parsimonious Bayesian Factor Analysis when the Number of Factors is Unknown},
	abstract = {We introduce a new and general set of identifiability conditions for factor models which handles the ordering problem associated with current common practice. In addition, the new class of parsimonious Bayesian factor analysis leads to a factor loading matrix repre- sentation which is an intuitive and easy to implement factor selection scheme. We argue that the structuring the factor loadings matrix is in concordance with recent trends in applied factor analysis. Our MCMC scheme for posterior inference makes several improvements over the existing alternatives while outlining various strategies for conditional posterior in- ference in a factor selection scenario. Four applications, two based on synthetic data and two based on well known real data, are introduced to illustrate the applicability and gener- ality of the new class of parsimonious factor models, as well as to highlight features of the proposed sampling schemes. Keywords:},
	author = {Fr{\"{u}}hwirth-Schnatter, S. and Lopes, H. F. },
	file = {:C$\backslash$:/Users/Windows/Documents/Claire IMIFA/Papers {\&} Books/SchnatterLopes.pdf:pdf},
	keywords = {cholesky decomposition,defficiency,fractional prior,heywood,heywood problem,hierarchical model,identifiability,parsimony,rank,rank defficiency},
	mendeley-groups = {IMIFA},
	number = {July 2015},
	pages = {1--37},
	year = {2010}
}

@INBOOK{Fruhwirth-Schnatter2011,
	title = {Dealing with label switching under model uncertainty}, 
	author = {Fr{\"{u}}hwirth-Schnatter, S.},
	publisher = {Wiley},
	year = {2011},
	isbn = {ISBN-10: 11199938},
	address = {Chichester},
	publisher = {Wiley},
	language = {EN},
	pages = {193-218}, 
	series = {Mixture estimation and applications},
	abstract = {Statistical mixture distributions are used to model scenarios in which certain variables are measured but a categorical variable is missing. For example, although clinical data on a patient may be available their disease category may not be, and this adds significant degrees of complication to the statistical analysis. The above situation characterises the simplest mixture-type scenario; variations include, among others, hidden Markov models, in which the missing variable follows a Markov chain model, and latent structure models, in which the missing variable or variables represent model-enriching devices rather than real physical entities. In the title of the workshop the term `mixture' is taken to include these and other variations along with the simple mixture. The motivating factors for this three-day workshop are that research on inference and computational techniques for mixture-type models is currently experiencing major advances and that simultaneously the application of mixture modelling to many fields in science and elsewhere has never been so rich. We thus assembling top players, from statistics and computer science, in both methodological research and applied inference at this fertile interface. The methodological component will involve both Bayesian and non-Bayesian contributions and biology and economics will feature strongly among the application areas to be covered.},
}

@techreport{Ghahramani1996,
  title={The EM algorithm for mixtures of factor analyzers},
  author={Ghahramani, Z. and Hinton, G. E.},
  year={1996},
}

@book{GMRFbook,
	AUTHOR = {H. Rue and L. Held},
	TITLE = {Gaussian {M}arkov Random Fields: {T}heory and Applications},
	SERIES = {Monographs on Statistics and Applied Probability},
	VOLUME = {104},
	PUBLISHER = {Chapman \& Hall},
	ADDRESS = {London},
	YEAR = 2005
}

@article{Hastie2014,
abstract = {We consider the question of Markov chain Monte Carlo sampling from a general stick-breaking Dirichlet process mixture model, with concentration parameter $\alpha$. This paper introduces a Gibbs sampling algorithm that combines the slice sampling approach of Walker (Communications in Statistics - Simulation and Computation 36:45–54, 2007) and the retrospective sampling approach of Papaspiliopoulos and Roberts (Biometrika 95(1):169–186, 2008). Our general algorithm is implemented as efficient open source C++ software, available as an R package, and is based on a blocking strategy similar to that suggested by Papaspiliopoulos (A note on posterior sampling from Dirichlet mixture models, 2008) and implemented by Yau et al. (Journal of the Royal Statistical Society, Series B (Statistical Methodology) 73:37–57, 2011). We discuss the difficulties of achieving good mixing in MCMC samplers of this nature in large data sets and investigate sensitivity to initialisation. We additionally consider the challenges when an additional layer of hierarchy is added such that joint inference is to be made on $\alpha$. We introduce a new label-switching move and compute the marginal partition posterior to help to surmount these difficulties. Our work is illustrated using a profile regression (Molitor et al. Biostatistics 11(3):484–498, 2010) application, where we demonstrate good mixing behaviour for both synthetic and real examples.},
archivePrefix = {arXiv},
arxivId = {1304.1778},
author = {Hastie, D. I. and Liverani, S. and Richardson, S.},
doi = {10.1007/s11222-014-9471-3},
eprint = {1304.1778},
file = {:C$\backslash$:/Users/Keefe/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie, Liverani, Richardson - 2014 - Sampling from Dirichlet process mixture models with unknown concentration parameter mixing issues.pdf:pdf},
isbn = {1122201494},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Bayesian clustering,Dirichlet process,Mixture model,Profile regression},
mendeley-groups = {IMIFA},
number = {5},
pages = {1023--1037},
pmid = {26321800},
title = {{Sampling from Dirichlet process mixture models with unknown concentration parameter: mixing issues in large data implementations}},
volume = {25},
year = {2014}
}

@ARTICLE{Ishwaran2001,
	title = {Bayesian Model Selection in Finite Mixtures by Marginal Density Decompositions},
	author = {Ishwaran, H. and James, L.F. and Sun, J.},
	year = {2001},
	journal = {Journal of the American Statistical Association},
	volume = {96},
	pages = {1316-1332},
	url = {http://EconPapers.repec.org/RePEc:bes:jnlasa:v:96:y:2001:m:december:p:1316-1332}
}

@article{Kalli2011,
abstract = {We propose a more efficient version of the slice sampler for Dirichlet process mixture models described by Walker (Commun. Stat., Simul. Comput. 36:45--54, 2007). This new sampler allows for the fitting of infinite mixture models with a wide-range of prior specifications. To illustrate this flexibility we consider priors defined through infinite sequences of independent positive random variables. Two applications are considered: density estimation using mixture models and hazard function estimation. In each case we show how the slice efficient sampler can be applied to make inference in the models. In the mixture case, two submodels are studied in detail. The first one assumes that the positive random variables are Gamma distributed and the second assumes that they are inverse-Gaussian distributed. Both priors have two hyperparameters and we consider their effect on the prior distribution of the number of occupied clusters in a sample. Extensive computational comparisons with alternative ``conditional'' simulation techniques for mixture models using the standard Dirichlet process prior and our new priors are made. The properties of the new priors are illustrated on a density estimation problem.},
author = {Kalli, M. and Griffin, J. E. and Walker, S. G.},
doi = {10.1007/s11222-009-9150-y},
file = {:D$\backslash$:/Dropbox/UCD/IMIFA/Papers {\&} Books/Kalli-Griffin-Walker-2011.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Dirichlet process,Hazard function,Markov chain Monte Carlo,Mixture model,Normalized weights,Slice sampler},
mendeley-groups = {IMIFA},
number = {1},
pages = {93--105},
title = {{Slice sampling mixture models}},
volume = {21},
year = {2011}
}

@book{McLachlanPeel,
	title = "Finite mixture models",
	author = "McLachlan, G. J. and Peel, D.",
	series = "Wiley series in probability and statistics",
	publisher = "J. Wiley \& Sons",
	address = "New York",
	url = "http://opac.inria.fr/record=b1097397",
	isbn = "0471006262",
	year = 2000
}

@article{McNicholas2008,
author = {McNicholas, P. D. and Murphy, T. B.},
file = {:C$\backslash$:/Users/Keefe/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McNicholas, Murphy - 2008 - Parsimonious Gaussian Mixture Models.pdf:pdf},
journal = {Statistics and Computing},
keywords = {analysis,cluster analysis,factor analysis,mixture models,probabilistic principal components},
mendeley-groups = {IMIFA},
number = {3},
pages = {285--296},
title = {{Parsimonious Gaussian Mixture Models}},
volume = {18},
year = {2008}
}

@article{Neal2000,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/1390653},
 abstract = {This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors.},
 author = {R. M. Neal},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {249-265},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Markov Chain Sampling Methods for Dirichlet Process Mixture Models},
 volume = {9},
 year = {2000}
}

@article{Papaspiliopoulos2008,
abstract = {Inference for Dirichlet process hierarchical models is typically performed using Markov chain Monte Carlo methods, which can be roughly categorized into marginal and conditional methods. The former integrate out analytically the infinite-dimensional component of the hierarchical model and sample from the marginal distribution of the remaining variables using the Gibbs sampler. Conditional methods impute the Dirichlet process and update it as a component of the Gibbs sampler. Since this requires imputation of an infinite-dimensional process, implementation of the conditional method has relied on finite approximations. In this paper, we show how to avoid such approximations by designing two novel Markov chain Monte Carlo algorithms which sample from the exact posterior distribution of quantities of interest. The approximations are avoided by the new technique of retrospective sampling. We also show how the algorithms can obtain samples from functionals of the Dirichlet process. The marginal and the conditional methods are compared and a careful simulation study is included, which involves a non-conjugate model, different datasets and prior specifications.},
archivePrefix = {arXiv},
arxivId = {0710.4228},
author = {Papaspiliopoulos, O. and Roberts, G. O.},
doi = {10.1093/biomet/asm086},
eprint = {0710.4228},
file = {:C$\backslash$:/Users/Keefe/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Papaspiliopoulos, Roberts - 2008 - Retrospective Markov chain Monte Carlo methods for Dirichlet process hierarchical models.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Exact simulation,Label switching,Mixture model,Retrospective sampling,Stick-breaking prior},
mendeley-groups = {IMIFA},
number = {1},
pages = {169--186},
title = {{Retrospective Markov chain Monte Carlo methods for Dirichlet process hierarchical models}},
volume = {95},
year = {2008}
}

@article{Peel2000,
abstract = {Normal mixture models are being increasingly used to model the distributions of a wide variety of random phenomena and to cluster sets of continuous multivariate data. However, for a set of data containing a group or groups of observations with longer than normal tails or atypical observations, the use of normal components may unduly affect the fit of the mixture model. In this paper, we consider a more robust approach by modelling the data by a mixture of t distributions. The use of the ECM algorithm to fit this t mixture model is described and examples of its use are given in the context of clustering multivariate data in the presence of atypical observations in the form of background noise.},
author = {Peel, D. and Mclachlan, G. J.},
doi = {http://dx.doi.org/10.1023/A:1008981510081},
file = {:C$\backslash$:/Users/Keefe/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peel, Mclachlan - 2000 - Robust mixture modelling using the t distribution.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {EM algorithm,cluster analysis,finite mixture models,maximum like-lihood,multivariate t components,normal components},
mendeley-groups = {IMIFA},
pages = {339--348},
title = {{Robust mixture modelling using the t distribution}},
volume = {10},
year = {2000}
}

@Article{Perman1992,
author="Perman, M.
and Pitman, J.
and Yor, M.",
title="Size-biased sampling of Poisson point processes and excursions",
journal="Probability Theory and Related Fields",
year="1992",
volume="92",
number="1",
pages="21--39",
abstract="Some general formulae are obtained for size-biased sampling from a Poisson point process in an abstract space where the size of a point is defined by an arbitrary strictly positive function. These formulae explain why in certain cases (gamma and stable) the size-biased permutation of the normalized jumps of a subordinator can be represented by a stickbreaking (residual allocation) scheme defined by independent beta random variables. An application is made to length biased sampling of excursions of a Markov process away from a recurrent point of its statespace, with emphasis on the Brownian and Bessel cases when the associated inverse local time is a stable subordinator. Results in this case are linked to generalizations of the arcsine law for the fraction of time spent positive by Brownian motion.",
issn="1432-2064",
doi="10.1007/BF01205234",
url="http://dx.doi.org/10.1007/BF01205234"
}



@article{Raftery2007,
	abstract = {The integrated likelihood (also called the marginal likelihood or the normalizing constant) is a central quantity in Bayesian model selection and model averaging. It is defined as the integral over the parameter space of the likelihood times the prior density. The Bayes factor for model comparison and Bayesian testing is a ratio of integrated likelihoods, and the model weights in Bayesian model averaging are proportional to the integrated likelihoods. We consider the estimation of the integrated likelihood from posterior simulation output, aiming at a generic method that uses only the likelihoods from the posterior simulation iterations. The key is the harmonic mean identity, which says that the reciprocal of the integrated likelihood is equal to the posterior harmonic mean of the likelihood. The simplest estimator based on the identity is thus the harmonic mean of the likelihoods. While this is an unbiased and simulation-consistent estimator, its reciprocal can have infinite variance and so it is unstable in general. We describe two methods for stabilizing the harmonic mean estimator. In the first one, the parameter space is reduced in such a way that the modified estimator involves a harmonic mean of heavier-tailed densities, thus resulting in a finite variance estimator. The resulting estimator is stable. It is also self-monitoring, since it obeys the central limit theorem, and so confidence intervals are available. We discuss general conditions under which this reduction is applicable.},
	author = {Raftery, A. E. and  Newton, M. and Krivitsky, P. N. and Satagopan, J. M.},
	file = {:C$\backslash$:/Users/Windows/Documents/Claire IMIFA/Papers {\&} Books/Raftery-BICM.pdf:pdf},
	journal = {Bayesian Statistics},
	keywords = {Raftery2007},
	mendeley-groups = {IMIFA},
	number = {8},
	pages = {1--45},
	title = {{Estimating the Integrated Likelihood via Posterior Simulation Using the Harmonic Mean Identity}},
	year = {2007}
}

@misc{Sethuraman1994,
abstract = {In this paper we give a simple and new constructive definition of Dirichlet measures removing the restriction that the basic space should be Rk. We also give complete, self contained proofs of the three basic results for Dirichlet measures: 1. The Dirichlet measure is a probability measure on the space of all probability measures. 2. It gives probability one to the subset of discrete probability measures. 3. The posterior distribution is also a Dirichlet measure.},
author = {Sethuraman, J.},
booktitle = {Statistica Sinica},
doi = {***},
file = {:C$\backslash$:/Users/Keefe/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sethuraman - 1994 - A constructive definition of Dirichlet priors.pdf:pdf},
mendeley-groups = {IMIFA},
pages = {639--650},
title = {{A constructive definition of Dirichlet priors}},
url = {http://www3.stat.sinica.edu.tw/statistica/j4n2/j4n27/..$\backslash$j4n216$\backslash$j4n216.htm},
volume = {4},
year = {1994}
}

@article{Viroli2011,
abstract = {Matrix-variate distributions represent a natural way for modeling random matrices. Realizations from random matrices are generated by the simultaneous observation of variables in different situations or locations, and are commonly arranged in three-way data structures. Among the matrix-variate distributions, the matrix normal density plays the same pivotal role as the multivariate normal distribution in the family of multivariate distributions. In this work we define and explore finite mixtures of matrix normals. An EM algorithm for the model estimation is developed and some useful properties are demonstrated. We finally show that the proposed mixture model can be a powerful tool for classifying three-way data both in supervised and unsupervised problems. A simulation study and some real examples are presented.},
author = {Viroli, C.},
doi = {10.1007/s11222-010-9188-x},
file = {:D$\backslash$:/Dropbox/UCD/IMIFA/Papers {\&} Books/Cinzia Stuff/Matrix Normals/art{\%}3A10.1007{\%}2Fs11222-010-9188-x.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {EM-algorithm,Model based clustering,Random matrix,Three-way data},
mendeley-groups = {IMIFA},
number = {4},
pages = {511--522},
title = {{Finite mixtures of matrix normal distributions for classifying three-way data}},
volume = {21},
year = {2011}
}

@article{Walker2007,
author = {Walker, S. G.},
title = {Sampling the Dirichlet Mixture Model with Slices},
journal = {Communications in Statistics - Simulation and Computation},
volume = {36},
number = {1},
pages = {45-54},
year = {2007},
doi = {10.1080/03610910601096262},

URL = { 
        http://dx.doi.org/10.1080/03610910601096262
    
},
eprint = { 
        http://dx.doi.org/10.1080/03610910601096262
    
}
,
    abstract = { We provide a new approach to the sampling of the well known mixture of Dirichlet process model. Recent attention has focused on retention of the random distribution function in the model, but sampling algorithms have then suffered from the countably infinite representation these distributions have. The key to the algorithm detailed in this article, which also keeps the random distribution functions, is the introduction of a latent variable which allows a finite number, which is known, of objects to be sampled within each iteration of a Gibbs sampler. }
}

@article{Wang2007,
  author = {Q. Wang and C. M. Carvalho and J. E. Lucas and M. West},
  title = {BFRM: Bayesian factor regression modelling},
  journal = {Bulletin of the International Society for Bayesian Analysis},
  year = {2007},
  volume = {14},
  pages = {4-5},
  url = {http://www.isds.duke.edu/ mw/.downloads/bfrm-isbabull07.pdf}
}

@article{West1992,
abstract = {INTRODUCTION Escobar and West (1991) develop Gibbs sampling methods for the computations involved in Bayesian density estimation and problems of deconvolution using mixtures of Dirichlet processes. We adopt the normal mixture model of this reference for example here - note that the assumed normal data distributions may be replaced by any exponential family form, if required. In analysis of random samples from these nonparametric Bayesian models, data y 1 ; : : : ; y n are assumed conditionally ...},
author = {West, M.},
file = {:C$\backslash$:/Users/Keefe/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/West - 1992 - Hyperparameter estimation in Dirichlet process mixture models.pdf:pdf},
journal = {ISDS discussion paper series},
mendeley-groups = {IMIFA},
pages = {{\#}92--03},
title = {{Hyperparameter estimation in Dirichlet process mixture models}},
url = {http://www.stat.duke.edu/{~}mw/.downloads/DP.learnalpha.pdf},
year = {1992}
}

@inproceedings{West2003,
  author = {M. West},
  title = {Bayesian factor regression models in the lqlq large p, small n" paradigm},
  booktitle = {Bayesian Statistics 7},
  publisher = {Oxford University Press},
  year = {2003},
  pages = {723-732},
  url = {http://ftp.isds.duke.edu/WorkingPapers/02-12.html}
}

@article{Yu2009,
abstract = {The Neal's review paper [4] presents three algorithms of Gibbs sampling for Dirichlet Process Mixture Models (DPMM) when conjugate priors are used. But he did not give technical details for these algorithms and thus it is unclear how to use them in a practical problem for a newbie. Rasmussen's paper [6], Teh's tutorial course [9] and Ranganathan's thesis appendix [5] provide us more technical details of these algorithms. Sudderth has a good review about the two Gibbs sampling methods with Chinese restaurant process in the Chapter 2 of his thesis[8]. The purpose of this report is thus to combine these materials into a self-contained tutorial for the techniques of Gibbs sampling for Dirichlet Process Mixture Models (DPMM), especially when conjugate priors are used. Later on, I found several papers, which are perhaps the original papers about the algorithms described here: ? The paper by Escobar and West [1] related to Section 4.1 ? The paper byWest, Muller and Escobar [11] seems to be related to Section 4.2 ? The paper by MacEarchern [3] seems to be related to Section 4.3 The papers [11, 3] also gives examples on normal distribution, which would be very helpful to understand the algorithms. But I have no time to read these papers in details. So this report is still based on Neal, Rasmussen, Teh, Ranganathan, and Sudderth's materials.},
author = {Yu, X.},
file = {:D$\backslash$:/Dropbox/UCD/IMIFA/Papers {\&} Books/technical details in Gibbs sampling for DP mixture model.pdf:pdf},
mendeley-groups = {IMIFA},
pages = {1--18},
title = {{Gibbs Sampling Methods for Dirichlet Process Mixture Model: Technical Details}},
year = {2009}
}

